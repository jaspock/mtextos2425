
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>T4. Repositorios y Tecnolog√≠as de Modelos (Generativos) Preentrenados &#8212; Miner√≠a de Textos</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/estilos.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="_static/logo-master-ca.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Miner√≠a de Textos</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Materiales de Miner√≠a de Textos
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Bloque T√âCNICAS
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="bloque2.html">
   1. T√©cnicas para la miner√≠a de textos
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Bloque PLN
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="bloque1.html">
   2. Introducci√≥n. Miner√≠a de textos y procesamiento del lenguaje natural
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Utilidades
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="doc_utils/creditosColab/web.html">
   3. Uso de Google Colab con una VM personalizada
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="_sources/bloque3_t4_centralizacion.md"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.md</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#introduccion">
   Introducci√≥n
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#repositorio-de-datasets">
   Repositorio de Datasets
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#listar-datasets-disponibles-en-el-repositorio">
     Listar datasets disponibles en el repositorio
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#como-cargar-datasets">
     ¬øC√≥mo cargar datasets?
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#tareas-subtareas-e-idiomas-de-datasets">
     Tareas, subtareas e idiomas de datasets
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#repositorio-de-modelos-pre-entrenados">
   Repositorio de Modelos pre-entrenados
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#listado-de-pipelines">
     Listado de Pipelines
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#como-buscar-y-reutilizar-modelos-pre-entrenados-en-la-plataforma">
     ¬øC√≥mo buscar y reutilizar modelos pre-entrenados en la plataforma?
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#configuraciones-de-modelos-transformers">
     Configuraciones de modelos transformers
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#tecnologias-de-generacion">
   Tecnolog√≠as de generaci√≥n
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gpt">
     GPT
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#entrenamiento-del-gpt">
       Entrenamiento del GPT
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#ventajas">
       Ventajas
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#desventajas">
       Desventajas
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#decodificacion-de-gpt">
       Decodificaci√≥n de GPT
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#copilot">
     Copilot
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id1">
       Ventajas
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id2">
       Desventajas
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#alternativas-a-copilot">
       Alternativas a Copilot
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#chatgpt">
     ChatGPT
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#ejemplo-de-uso-de-la-api-chatgpt">
       Ejemplo de uso de la API ChatGPT:
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id3">
       Ventajas
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id4">
       Desventajas
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#alternativas-a-chatgpt">
       Alternativas a ChatGPT
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#metodos-para-optimizar-modelos-preentrenados">
     M√©todos para Optimizar Modelos Preentrenados
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#ingenieria-de-prompts-prompt-engineering">
       Ingenier√≠a de prompts (
       <em>
        Prompt Engineering
       </em>
       )
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#ajuste-de-prompts-prompt-tuning">
       Ajuste de prompts (
       <em>
        Prompt Tuning
       </em>
       )
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#diferencia-entre-ajuste-de-prompts-y-ajuste-fino">
         Diferencia entre ajuste de prompts y ajuste fino
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#ajuste-fino-fine-tuning">
       Ajuste fino (
       <em>
        Fine-tuning
       </em>
       )
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#continual-pre-training">
         Continual pre-training
        </a>
       </li>
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#fine-tuning-lora">
         Fine-tuning LoRA
        </a>
       </li>
      </ul>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#instruccion-de-modelos">
     Instrucci√≥n de modelos
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#diferencia-entre-instruccion-de-modelos-y-ajuste-de-prompts">
       Diferencia entre instrucci√≥n de modelos y ajuste de prompts
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#flan">
       FLAN
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#cuaderno-de-ejemplo-de-instruction-tuning">
       Cuaderno de ejemplo de instruction-tuning
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bibliografia">
   Bibliograf√≠a
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>T4. Repositorios y Tecnolog√≠as de Modelos (Generativos) Preentrenados</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#introduccion">
   Introducci√≥n
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#repositorio-de-datasets">
   Repositorio de Datasets
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#listar-datasets-disponibles-en-el-repositorio">
     Listar datasets disponibles en el repositorio
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#como-cargar-datasets">
     ¬øC√≥mo cargar datasets?
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#tareas-subtareas-e-idiomas-de-datasets">
     Tareas, subtareas e idiomas de datasets
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#repositorio-de-modelos-pre-entrenados">
   Repositorio de Modelos pre-entrenados
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#listado-de-pipelines">
     Listado de Pipelines
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#como-buscar-y-reutilizar-modelos-pre-entrenados-en-la-plataforma">
     ¬øC√≥mo buscar y reutilizar modelos pre-entrenados en la plataforma?
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#configuraciones-de-modelos-transformers">
     Configuraciones de modelos transformers
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#tecnologias-de-generacion">
   Tecnolog√≠as de generaci√≥n
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gpt">
     GPT
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#entrenamiento-del-gpt">
       Entrenamiento del GPT
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#ventajas">
       Ventajas
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#desventajas">
       Desventajas
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#decodificacion-de-gpt">
       Decodificaci√≥n de GPT
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#copilot">
     Copilot
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id1">
       Ventajas
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id2">
       Desventajas
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#alternativas-a-copilot">
       Alternativas a Copilot
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#chatgpt">
     ChatGPT
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#ejemplo-de-uso-de-la-api-chatgpt">
       Ejemplo de uso de la API ChatGPT:
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id3">
       Ventajas
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id4">
       Desventajas
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#alternativas-a-chatgpt">
       Alternativas a ChatGPT
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#metodos-para-optimizar-modelos-preentrenados">
     M√©todos para Optimizar Modelos Preentrenados
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#ingenieria-de-prompts-prompt-engineering">
       Ingenier√≠a de prompts (
       <em>
        Prompt Engineering
       </em>
       )
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#ajuste-de-prompts-prompt-tuning">
       Ajuste de prompts (
       <em>
        Prompt Tuning
       </em>
       )
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#diferencia-entre-ajuste-de-prompts-y-ajuste-fino">
         Diferencia entre ajuste de prompts y ajuste fino
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#ajuste-fino-fine-tuning">
       Ajuste fino (
       <em>
        Fine-tuning
       </em>
       )
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#continual-pre-training">
         Continual pre-training
        </a>
       </li>
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#fine-tuning-lora">
         Fine-tuning LoRA
        </a>
       </li>
      </ul>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#instruccion-de-modelos">
     Instrucci√≥n de modelos
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#diferencia-entre-instruccion-de-modelos-y-ajuste-de-prompts">
       Diferencia entre instrucci√≥n de modelos y ajuste de prompts
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#flan">
       FLAN
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#cuaderno-de-ejemplo-de-instruction-tuning">
       Cuaderno de ejemplo de instruction-tuning
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bibliografia">
   Bibliograf√≠a
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="t4-repositorios-y-tecnologias-de-modelos-generativos-preentrenados">
<h1>T4. Repositorios y Tecnolog√≠as de Modelos (Generativos) Preentrenados<a class="headerlink" href="#t4-repositorios-y-tecnologias-de-modelos-generativos-preentrenados" title="Permalink to this headline">#</a></h1>
<div class="note admonition">
<p class="admonition-title">Nota</p>
<p>Lee con atenci√≥n el tema 4 del bloque 3. Realiza las lecturas propuestas y finalmente contesta el cuestionario que encontrar√°s en la secci√≥n de evaluaci√≥n relativo a este tema, el cual se encuentra en el √≠ndice del bloque 3.  En la clase presencial repasaremos los conceptos te√≥ricos principales correspondientes a la sesi√≥n. <strong>Apertura el 10/04/2025- Cierre 23:59 del 07/05/2025</strong> (el d√≠a anterior a la clase presencial).</p>
<p>Tiempo de dedicaci√≥n: 3 horas (as√≠ncrona) + 2 horas trabajo independiente</p>
</div>
<p>Contenidos:</p>
<ul class="simple">
<li><p><a class="reference external" href="#introduccion">Huggingface: Introducci√≥n</a></p></li>
<li><p><a class="reference external" href="#repositorio-de-datasets">Huggingface: Repositorio de Datasets</a></p></li>
<li><p><a class="reference external" href="#repositorio-de-modelos-pre-entrenados">Huggingface: Repositorio de Modelos pre-entrenados</a></p></li>
<li><p><a class="reference external" href="#tecnologias-de-generacion">Tecnolog√≠as de Generaci√≥n</a></p></li>
</ul>
<section id="introduccion">
<h2>Introducci√≥n<a class="headerlink" href="#introduccion" title="Permalink to this headline">#</a></h2>
<p><a class="reference external" href="http://Huggingface.co">Huggingface.co</a> una compa√±√≠a centrada en el PLN la cual ha desarrollado las <a class="reference external" href="https://huggingface.co/transformers/"><strong>librer√≠as Transformers</strong></a>, <strong>centralizado datasets</strong> y ha <strong>creado modelos de aprendizaje pre-entrenados</strong> disponibles a trav√©s de sus librer√≠as de programaci√≥n.
Las librer√≠as de Huggingface actualmente dan soporte a empresas muy importantes del mercado tecnol√≥gico. Ver <a class="reference external" href="https://huggingface.co/">https://huggingface.co/</a>.</p>
</section>
<section id="repositorio-de-datasets">
<h2>Repositorio de Datasets<a class="headerlink" href="#repositorio-de-datasets" title="Permalink to this headline">#</a></h2>
<p>Proporciona conjuntos de datos para muchas tareas de PLN como clasificaci√≥n de texto, respuesta a preguntas, modelado de lenguaje, etc.<br />
Instalaci√≥n de librer√≠a de manipulaci√≥n de datasets
Para la <strong>instalaci√≥n</strong> de la librer√≠a de manipulaci√≥n de datasets se debe ejecutar la siguiente instrucci√≥n pip:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">pip</span> <span class="n">install</span> <span class="n">datasets</span>
</pre></div>
</div>
<p>Para asegurarnos de que Transformers dataset se ha instalado correctamente es necesario ejecutar la siguiente instrucci√≥n:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">python</span> <span class="o">-</span><span class="n">c</span> <span class="s2">&quot;from datasets import load_dataset; print(load_dataset(&#39;squad&#39;, split=&#39;train&#39;)[0])&quot;</span>

</pre></div>
</div>
<p>Esta instrucci√≥n debe descargar la versi√≥n 1 del conjunto de datos de respuesta a preguntas de Stanford, cargar su divisi√≥n de entrenamiento e imprimir el primer ejemplo de entrenamiento de la siguiente manera:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">{</span><span class="s1">&#39;id&#39;</span><span class="p">:</span> <span class="s1">&#39;5733be284776f41900661182&#39;</span><span class="p">,</span> <span class="s1">&#39;title&#39;</span><span class="p">:</span> <span class="s1">&#39;University_of_Notre_Dame&#39;</span><span class="p">,</span> <span class="s1">&#39;context&#39;</span><span class="p">:</span> <span class="s1">&#39;Architecturally, the school has a Catholic character. Atop the Main Building</span><span class="se">\&#39;</span><span class="s1">s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend &quot;Venite Ad Me Omnes&quot;...&#39;</span><span class="p">,</span> <span class="s1">&#39;question&#39;</span><span class="p">:</span> <span class="s1">&#39;To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?&#39;</span><span class="p">,</span> <span class="s1">&#39;answers&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;text&#39;</span><span class="p">:</span> <span class="n">array</span><span class="p">([</span><span class="s1">&#39;Saint Bernadette Soubirous&#39;</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">object</span><span class="p">),</span> <span class="s1">&#39;answer_start&#39;</span><span class="p">:</span> <span class="n">array</span><span class="p">([</span><span class="mi">515</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">int32</span><span class="p">)}}</span>
</pre></div>
</div>
<section id="listar-datasets-disponibles-en-el-repositorio">
<h3>Listar datasets disponibles en el repositorio<a class="headerlink" href="#listar-datasets-disponibles-en-el-repositorio" title="Permalink to this headline">#</a></h3>
<p>Para listar los conjuntos de datos disponibles es necesario ejecutar la siguiente funci√≥n <code class="docutils literal notranslate"><span class="pre">datasets.list_datasets()</span></code> que pertenece a la clase <code class="docutils literal notranslate"><span class="pre">datasets</span></code>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">list_datasets</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">datasets_list</span> <span class="o">=</span> <span class="n">list_datasets</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">len</span><span class="p">(</span><span class="n">datasets_list</span><span class="p">)</span>
<span class="go">656</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;, &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">dataset</span> <span class="k">for</span> <span class="n">dataset</span> <span class="ow">in</span> <span class="n">datasets_list</span><span class="p">))</span>
<span class="go">aeslc, ag_news, ai2_arc, allocine, anli, arcd, art, billsum, blended_skill_talk, blimp, blog_authorship_corpus, bookcorpus, boolq, break_data,</span>
<span class="go">c4, cfq, civil_comments, cmrc2018, cnn_dailymail, coarse_discourse, com_qa, commonsense_qa, compguesswhat, coqa, cornell_movie_dialog, cos_e,</span>
<span class="go">cosmos_qa, crime_and_punish, csv, definite_pronoun_resolution, discofuse, docred, drop, eli5, empathetic_dialogues, eraser_multi_rc, esnli,</span>
<span class="go">event2Mind, fever, flores, fquad, gap, germeval_14, ghomasHudson/cqc, gigaword, glue, ‚Ä¶</span>
</pre></div>
</div>
<p>Otra alternativa es:</p>
<ol class="simple">
<li><p>Ir a la web <a class="reference external" href="https://huggingface.co">https://huggingface.co</a></p></li>
<li><p>Seleccionar el men√∫ Datasets: <a class="reference external" href="https://huggingface.co/datasets">https://huggingface.co/datasets</a></p></li>
<li><p>Filtrar por categor√≠a, idioma, tarea y/o licencia</p></li>
</ol>
</section>
<section id="como-cargar-datasets">
<h3>¬øC√≥mo cargar datasets?<a class="headerlink" href="#como-cargar-datasets" title="Permalink to this headline">#</a></h3>
<p>Haciendo uso de la funci√≥n <code class="docutils literal notranslate"><span class="pre">load_dataset</span></code> se nos permite recuperar cualquier dataset registrado en el repositorio. Por ejemplo, el dataset <strong>MRPC</strong> que ha sido proporcionado en el √≠ndice de referencia GLUE (<a class="reference external" href="https://gluebenchmark.com/leaderboard">https://gluebenchmark.com/leaderboard</a>).</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_dataset</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dataset</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s1">&#39;glue&#39;</span><span class="p">,</span> <span class="s1">&#39;mrpc&#39;</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="s1">&#39;train&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>O podemos ver otro ejemplo como el de <a class="reference external" href="https://knowledge-learning.github.io/ehealthkd-2020/"><strong>eHealth-KD</strong></a></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_dataset</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dataset</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">&quot;ehealth_kd&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>No obstante, la librer√≠a <code class="docutils literal notranslate"><span class="pre">datasets</span></code> permite adem√°s <strong>cargar conjuntos de datos propios</strong> que no formen parte del repositorio. Por ejemplo:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_dataset</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dataset</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s1">&#39;csv&#39;</span><span class="p">,</span> <span class="n">data_files</span><span class="o">=</span><span class="s1">&#39;my_file.csv&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>Para m√°s detalles sobre las distintas funciones y par√°metros permitidos para manipular datasets ver la siguiente documentaci√≥n:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://huggingface.co/docs/datasets/v1.5.0/quicktour.html">https://huggingface.co/docs/datasets/v1.5.0/quicktour.html</a></p></li>
<li><p><a class="reference external" href="https://huggingface.co/docs/datasets/quickstart">https://huggingface.co/docs/datasets/quickstart</a></p></li>
</ul>
</section>
<section id="tareas-subtareas-e-idiomas-de-datasets">
<h3>Tareas, subtareas e idiomas de datasets<a class="headerlink" href="#tareas-subtareas-e-idiomas-de-datasets" title="Permalink to this headline">#</a></h3>
<p><strong>Categor√≠as:</strong>
En este repositorio podemos encontrar un amplio catalogo de tareas (categor√≠as) por las cuales filtrar y especificar el tipo de dateset que estamos buscando. Hemos de resaltar que estos datasets existen originalmente en diferentes formatos, nos obstante en una vez incluido en este repositorio, el formato es estandar. Por tal motivo, a trav√©s de las libr√≠as de manipulaci√≥n (las mencionadas enteriormente) que ofrece Huggingface, podemos acceder a ellos y gestionarlos. Adem√°s, estos datasets se encuentran caracterizados por <strong>idioma</strong>, <strong>subtarea</strong> en la que se puede utilizar y la <strong>licencia de uso</strong>.</p>
<a class="bg-primary mb-1 reference internal image-reference" href="_images/hf_dataset_categoria.jpg"><img alt="comic xkcd 2421" class="bg-primary mb-1 align-center" src="_images/hf_dataset_categoria.jpg" style="width: 300px;" /></a>
<p>Figura 1. Categor√≠as generales (o tareas) que permiten el filtro de datasets</p>
<p><strong>M√°s de 134 tareas y m√°s de 194 idiomas:</strong></p>
<a class="bg-primary mb-1 reference internal image-reference" href="_images/hf_dataset_tareas_idiomas.jpg"><img alt="comic xkcd 2421" class="bg-primary mb-1 align-center" src="_images/hf_dataset_tareas_idiomas.jpg" style="width: 300px;" /></a>
<p>Figura 2. Idiomas de los datasets</p>
</section>
</section>
<section id="repositorio-de-modelos-pre-entrenados">
<h2>Repositorio de Modelos pre-entrenados<a class="headerlink" href="#repositorio-de-modelos-pre-entrenados" title="Permalink to this headline">#</a></h2>
<p>La biblioteca de Transformers permite el <strong>uso de modelos previamente entrenados</strong> para tareas de Comprensi√≥n del lenguaje natural (NLU), i.e. como analizar el sentimiento de un texto, y Generaci√≥n del lenguaje natural (NLG), i.e. como completar un mensaje con texto nuevo o traducir a otro idioma.
A groso modo listamos los modelos que nos podemos encontrar</p>
<ul class="simple">
<li><p><strong>An√°lisis de sentimiento</strong>: Conocer si un texto es positivo o negativo</p></li>
<li><p><strong>Generaci√≥n de texto</strong> (en ingl√©s): proporcionar un mensaje para el cual el modelo generar√° un texto.</p></li>
<li><p><strong>Reconocimiento de entidades nombradas</strong> (NER): Dado en una oraci√≥n de entrada se etiqueta cada palabra con la entidad que esta representa (persona, lugar, organizaci√≥n, etc.)</p></li>
<li><p><strong>Respuesta a preguntas</strong>: Teniendo en cuenta un modelo de un contexto determinado, dado un pregunta se obtiene una respuesta.</p></li>
<li><p><strong>Relleno de texto con m√°scara</strong>: Dado un texto con palabras enmascaradas (p. Ej., Reemplazado por [M√ÅSCARA]), completar los espacios en blanco.</p></li>
<li><p><strong>Resumen</strong>: Generaci√≥n de un resumen a partir de texto extenso.</p></li>
<li><p><strong>Traducci√≥n</strong>: Traducci√≥n de un texto a otro idioma.</p></li>
<li><p><strong>Extracci√≥n de caracter√≠sticas</strong>: Obtener una representaci√≥n tensorial del texto.
Tomado de <a class="reference external" href="https://huggingface.co/transformers/quicktour.html">https://huggingface.co/transformers/quicktour.html</a></p></li>
</ul>
<p><strong>Listado de tareas tal y como las podemos encontrar en el repositorio:</strong>
El listado de tareas, como categor√≠as, en las que podemos filtar los distintos modelos preentrenados que ofrece el repositorio Huggingface, es igual de amplio que el de los datasets. Como podemos observar, a partir de 2022 tal y como se describe m√°s adelante, en el siguiente imagen este repositorio no solo ofrece modelos prentrenados para el modelado del lenguaje, sino tambi√©n para desarrollar tareas de distintas modalidades: multimodal, lenguaje, audio, visi√≥n (imagen), datos estructurados (tabulados), y otros.</p>
<a class="bg-primary mb-1 reference internal image-reference" href="_images/hf_modelos_tareas.jpg"><img alt="comic xkcd 2421" class="bg-primary mb-1 align-center" src="_images/hf_modelos_tareas.jpg" style="width: 300px;" /></a>
<p>Figura 3. Tareas filtro modelos</p>
<p><strong>Idiomas para los que se han entrenado los modelos:</strong>
El listado de idiomas,como categor√≠as, en las que podemos filtar los distintos modelos preentrenados que ofrece el repositorio Huggingface, es igual de amplio que el de los datasets.</p>
<p>Una explicaci√≥n detallada sobre cada una de estas tareas y ejemplos de uso con Huggingface Transformer la podemos encontrar en el siguiente enlace:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://huggingface.co/transformers/task_summary.html">https://huggingface.co/transformers/task_summary.html</a></p></li>
</ul>
<p><strong>Huggingface a partir de 2022</strong>
A mediados de 2022 esta plataforma federativa da un paso agigantado expandiendo datasets y modelos preentrenados de solo ofrecer recursos para la modalidad de Procesamiento del Lenguaje Natural, a ofrecer recursos Multimodales, Visi√≥n por Computadora, Procesamiento de Audio, Procesamiento de datos Tabulares y  para Aprendisaje por reforzamiento.</p>
<p>En la mayor√≠a de los casos se ofrece una ejemplo de uso y documentaci√≥n. Poner en marcha cualquiera de estas tareas, reajustando o no los modelos prentrenados que se ofrecen en esta plataforma, se encuentra bien documentado y ejemplificado en ella: Ver Categor√≠as <a class="reference external" href="https://huggingface.co/tasks">https://huggingface.co/tasks</a></p>
<a class="bg-primary mb-1 reference internal image-reference" href="_images/doc-tareas-hf.jpg"><img alt="comic xkcd 2421" class="bg-primary mb-1 align-center" src="_images/doc-tareas-hf.jpg" style="width: 600px;" /></a>
<p>Figura 4. Categor√≠as de documentaciones agrupadas por tareas y modalidades</p>
<p>Ejemplo de An√°lisis de Sentimientos con Huggingface Transformer:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">pipeline</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">classifier</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="s1">&#39;sentiment-analysis&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">classifier</span><span class="p">(</span><span class="s1">&#39;We are very happy to show you the ü§ó Transformers library.&#39;</span><span class="p">)</span>
<span class="go">[{&#39;label&#39;: &#39;POSITIVE&#39;, &#39;score&#39;: 0.9997795224189758}]</span>
</pre></div>
</div>
<p>Si os fij√°is hemos cargado un modelo pre-entrenado a trav√©s del pipeline  <code class="docutils literal notranslate"><span class="pre">sentiment-analysis</span></code> para utilizarlo como clasificador. Este <strong>modelo</strong> se puede <strong>reentrenar</strong> a escenarios espec√≠ficos si queremos realizando un ajuste sobre un nuevo corpus. Para <strong>m√°s detalles ver la clase pr√°ctica</strong> <a class="reference external" href="https://jaspock.github.io/mtextos2425/bloque3_p3_SA-Transformers-Training-FineTuning.html"><code class="docutils literal notranslate"><span class="pre">bloque3_p3_SA-Transformers-Training-FineTuning</span></code></a></p>
<p>Si queremos que el pipeline sea multilingue, podemos indicar el modelo exacto que contemple un diccionario de este tipo y el pipeline lo ensamblar√° internamente. Mirad el siguiente ejemplo:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">pipeline</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">classifier</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="s1">&#39;sentiment-analysis&#39;</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="s1">&#39;nlptown/bert-base-multilingual-uncased-sentiment&#39;</span> <span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">classifier</span><span class="p">(</span><span class="s1">&#39;Estoy muy triste&#39;</span><span class="p">)</span>
<span class="go">[{&#39;label&#39;: &#39;1 star&#39;, &#39;score&#39;: 0.7241697907447815}]</span>
</pre></div>
</div>
<p>Para otras tareas como Rellenado de M√°scaras podemos ver como podemos simplemente indicar el tipo de tarea para que el pipeline seleccione el tipo de configuraci√≥n m√°s adecuada a esta y el modelo que queremos aplicarle. Con solo cambiar el modelo base podemos hacer esta tarea unilingue a multilingue o cambiar de idioma. Ver el ejemplo a continuaci√≥n:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelWithLMHead</span><span class="p">,</span> <span class="n">AutoTokenizer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelWithLMHead</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;mrm8488/RuPERTa-base&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;mrm8488/RuPERTa-base&quot;</span><span class="p">,</span> <span class="n">do_lower_case</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">pipeline</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">pipeline_fill_mask</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="s2">&quot;fill-mask&quot;</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">pipeline_fill_mask</span><span class="p">(</span><span class="s2">&quot;Espa√±a es un pa√≠s muy &lt;mask&gt; en la UE&quot;</span><span class="p">)</span>

<span class="go">[{&#39;score&#39;: 0.19951821863651276,</span>
<span class="go">  &#39;sequence&#39;: &#39;Espa√±a es un pa√≠s muy importante en la UE&#39;,</span>
<span class="go">  &#39;token&#39;: 1560,</span>
<span class="go">  &#39;token_str&#39;: &#39; importante&#39;},</span>
<span class="go"> {&#39;score&#39;: 0.04137842729687691,</span>
<span class="go">  &#39;sequence&#39;: &#39;Espa√±a es un pa√≠s muy grande en la UE&#39;,</span>
<span class="go">  &#39;token&#39;: 2741,</span>
<span class="go">  &#39;token_str&#39;: &#39; grande&#39;},</span>
<span class="go"> {&#39;score&#39;: 0.029216745868325233,</span>
<span class="go">  &#39;sequence&#39;: &#39;Espa√±a es un pa√≠s muy peque√±o en la UE&#39;,</span>
<span class="go">  &#39;token&#39;: 2948,</span>
<span class="go">  &#39;token_str&#39;: &#39; peque√±o&#39;},</span>
<span class="go"> {&#39;score&#39;: 0.02563760057091713,</span>
<span class="go">  &#39;sequence&#39;: &#39;Espa√±a es un pa√≠s muy popular en la UE&#39;,</span>
<span class="go">  &#39;token&#39;: 5782,</span>
<span class="go">  &#39;token_str&#39;: &#39; popular&#39;},</span>
<span class="go"> {&#39;score&#39;: 0.022264542058110237,</span>
<span class="go">  &#39;sequence&#39;: &#39;Espa√±a es un pa√≠s muy antiguo en la UE&#39;,</span>
<span class="go">  &#39;token&#39;: 5240,</span>
<span class="go">  &#39;token_str&#39;: &#39; antiguo&#39;}]</span>
</pre></div>
</div>
<section id="listado-de-pipelines">
<h3>Listado de Pipelines<a class="headerlink" href="#listado-de-pipelines" title="Permalink to this headline">#</a></h3>
<p>En Huggingface podemos encontrar una serie de Pipelines ya preparados para enfrentar tareas concretas a los cuales les podemos suministrar distintos modelos y tokenizadores transformes. Ver ejemplos: <a class="reference external" href="https://huggingface.co/transformers/main_classes/pipelines.html">https://huggingface.co/transformers/main_classes/pipelines.html</a></p>
</section>
<section id="como-buscar-y-reutilizar-modelos-pre-entrenados-en-la-plataforma">
<h3>¬øC√≥mo buscar y reutilizar modelos pre-entrenados en la plataforma?<a class="headerlink" href="#como-buscar-y-reutilizar-modelos-pre-entrenados-en-la-plataforma" title="Permalink to this headline">#</a></h3>
<p>A continuaci√≥n, se listan los pasos a seguir:</p>
<ol class="simple">
<li><p>Dirigirse al repositorio <a class="reference external" href="https://huggingface.co/">https://huggingface.co/</a></p></li>
<li><p>Seleccionar el men√∫ <code class="docutils literal notranslate"><span class="pre">models</span></code> que nos llevar√° a <a class="reference external" href="https://huggingface.co/models">https://huggingface.co/models</a></p></li>
<li><p>Filtrar el listado de modelos seg√∫n la tarea, idioma, librer√≠a (Pytorch o TensorFlow), dataset sobre el que fue entrenado, o licencia. Por ejemplo:  tarea <code class="docutils literal notranslate"><span class="pre">Text</span> <span class="pre">Classification</span></code>; idioma <code class="docutils literal notranslate"><span class="pre">es</span></code>.</p></li>
<li><p>Elegir un modelo de la lista. Por ejemplo: <code class="docutils literal notranslate"><span class="pre">bert-base-multilingual-uncased-sentiment</span></code></p></li>
<li><p>Obtendremos la documentaci√≥n necesaria para utilizar el modelo.</p></li>
</ol>
<p><strong>Conociendo el nombre del modelo</strong> a utilizar entonces podemos <strong>hacer uso</strong> de este a trav√©s de la librer√≠a <strong>Transformer</strong>.
En la propia documentaci√≥n se aporta el <strong>c√≥digo de ejemplo</strong> para hacer uso del modelo y en algunos casos una <a class="reference external" href="https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment"><strong>interfaz para probarlo</strong></a>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForSequenceClassification</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;nlptown/bert-base-multilingual-uncased-sentiment&quot;</span><span class="p">)</span>  <span class="c1"># cargando el toquenizador basado en el modelo preentrenado</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;nlptown/bert-base-multilingual-uncased-sentiment&quot;</span><span class="p">)</span> <span class="c1"># cargando del modelo preentrenado</span>
</pre></div>
</div>
</section>
<section id="configuraciones-de-modelos-transformers">
<h3>Configuraciones de modelos transformers<a class="headerlink" href="#configuraciones-de-modelos-transformers" title="Permalink to this headline">#</a></h3>
<p>Los <strong>modelos pre-entrenados</strong> que se brindan en el repositorio se <strong>basan</strong> en alguna de las <strong>arquitecturas Transformers</strong> descrita en la documentaci√≥n del repositorio (<a class="reference external" href="https://huggingface.co/docs">https://huggingface.co/docs</a>).
Si tomamos como referencia la arquitectura de modelo Transformer <a class="reference external" href="https://huggingface.co/transformers/model_doc/distilbert.html#overview">DistilBERT</a> podemos conocer c√≥mo <strong>gestionar</strong> los distintos <strong>par√°metros</strong>, <a class="reference external" href="https://huggingface.co/transformers/model_doc/distilbert.html#distilbertconfig"><strong>configuraciones de red neuronal</strong></a>, <a class="reference external" href="https://huggingface.co/transformers/model_doc/distilbert.html#distilberttokenizer"><strong>tokenizador</strong></a> y <strong>ejemplos documentados</strong> para cada tipo de tarea, tal y como podemos encontrar en el siguiente enlace (<a class="reference external" href="https://huggingface.co/course/chapter7/">https://huggingface.co/course/chapter7/</a>).</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># !pip install transformers</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">DistilBertTokenizer</span><span class="p">,</span> <span class="n">DistilBertModel</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">DistilBertTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;distilbert-base-uncased&#39;</span><span class="p">)</span> <span class="c1"># cargando de toquenizador basado en el modelo preentrenado</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">DistilBertModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;distilbert-base-uncased&#39;</span><span class="p">)</span> <span class="c1"># cargando el modelo preentrenado</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">&quot;Hello, my dog is cute&quot;</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">last_hidden_states</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">last_hidden_state</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">last_hidden_states</span><span class="p">)</span>

<span class="go">tensor([[[-1.8296e-01, -7.4054e-02,  5.0267e-02,  ..., -1.1261e-01,</span>
<span class="go">           4.4493e-01,  4.0941e-01],</span>
<span class="go">         [ 7.0631e-04,  1.4825e-01,  3.4328e-01,  ..., -8.6039e-02,</span>
<span class="go">           6.9475e-01,  4.3353e-02],</span>
<span class="go">         [-5.0721e-01,  5.3086e-01,  3.7163e-01,  ..., -5.6287e-01,</span>
<span class="go">           1.3756e-01,  2.8475e-01],</span>
<span class="go">         ...,</span>
<span class="go">         [-4.2251e-01,  5.7314e-02,  2.4338e-01,  ..., -1.5223e-01,</span>
<span class="go">           2.4462e-01,  6.4155e-01],</span>
<span class="go">         [-4.9384e-01, -1.8895e-01,  1.2641e-01,  ...,  6.3241e-02,</span>
<span class="go">           3.6913e-01, -5.8252e-02],</span>
<span class="go">         [ 8.3269e-01,  2.4948e-01, -4.5440e-01,  ...,  1.1998e-01,</span>
<span class="go">          -3.9257e-01, -2.7785e-01]]], grad_fn=&lt;NativeLayerNormBackward&gt;)</span>

</pre></div>
</div>
<p>Es importante conocer que las <strong>configuraciones</strong> de modelos Transformer ya <strong>cuentan</strong> con <strong>modelos base pre-entrenados</strong>. En el caso de <code class="docutils literal notranslate"><span class="pre">DistilBERT</span></code> podemos encontrar <code class="docutils literal notranslate"><span class="pre">distilbert-base-uncased</span></code>.</p>
</section>
</section>
<section id="tecnologias-de-generacion">
<h2>Tecnolog√≠as de generaci√≥n<a class="headerlink" href="#tecnologias-de-generacion" title="Permalink to this headline">#</a></h2>
<section id="gpt">
<h3>GPT<a class="headerlink" href="#gpt" title="Permalink to this headline">#</a></h3>
<p>GPT significa ‚ÄúGenerative Pretrained Transformer‚Äù. Es un modelo de lenguaje que utiliza t√©cnicas de deep learning para generar texto de manera aut√≥noma. GPT ha sido entrenado en una amplia cantidad de contenido textual. !Es <strong>orientado a liber√≠as</strong>! Es decir, se puede incorporar el componente en tu propia aplicaci√≥n.</p>
<ul class="simple">
<li><p>GPT-1: Es la primera versi√≥n de GPT, <a class="reference external" href="https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf">entrenado con 117 millones de par√°metros</a>. Aunque es significativamente m√°s limitada que las versiones posteriores, a√∫n es capaz de generar texto aceptable en muchos contextos.
La arquietectura de GPT-1 es principalmente un conjunto de 12 bloques de transformadores decodificadores colocados uno tras otro (ej. 12x ver la imagen). Los datos de texto se codifican mediante una <a class="reference external" href="https://arxiv.org/pdf/1508.07909.pdf">codificaci√≥n de pares de bytes</a> adaptada a caracteres. La <a class="reference external" href="https://arxiv.org/pdf/1706.03762.pdf">incrustaci√≥n de posici√≥n es aprendida, en lugar de la t√≠pica sinusoidal est√°tica</a>. La longitud m√°xima para tokens consecutivos es 512. La capa superior es simplemente una capa softmax adaptada a la tarea de aprendizaje espec√≠fica.</p></li>
<li><p>GPT-2: Es la segunda versi√≥n de GPT, con solo <a class="reference external" href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">1.5 mil millones de par√°metros</a>. Es capaz de generar texto coherente y a menudo convincente. GPT-2 tiene b√°sicamente la misma arquitectura que GPT-1, pero el modelo m√°s grande contiene 48 bloques(48x ver la imagen) de transformadores. La segunda capa de normalizaci√≥n se mueve a la primera posici√≥n en un bloque y el √∫ltimo bloque contiene una capa de normalizaci√≥n adicional. Los pesos se inicializan de forma ligeramente diferente y se aumenta el tama√±o del vocabulario. El n√∫mero de tokens consecutivos se incrementa a 1024.</p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2005.14165">GPT-3</a>: Es la tercera versi√≥n de GPT y es uno de los modelos de lenguaje m√°s grandes y avanzados jam√°s entrenados. Tiene m√°s de <a class="reference external" href="https://arxiv.org/abs/2005.14165">175 mil millones de par√°metros</a>, lo que le permite generar texto muy convincente en una amplia variedad de contextos. GPT-3 tiene la misma arquitectura que GPT-2, pero el n√∫mero de bloques aument√≥ a 96 en el modelo m√°s grande y el tama√±o del contexto (n√∫mero de tokens consecutivos) aument√≥ a 2048. Las <a class="reference external" href="https://arxiv.org/pdf/1904.10509.pdf">capas de autoatenci√≥n de varios cabezales se alternan entre los t√≠picos densos los escasos y los dispersos</a>.</p></li>
<li><p>‚Ä¶</p></li>
</ul>
<p>GPT-1 se entrena de manera autosupervisada (aprende a predecir la siguiente palabra en datos de texto) y se ajusta de manera de aprendizaje supervisado. GPT-2 se entrena de forma totalmente autosupervisada, centr√°ndose en la transferencia de <em>zero-shot</em> y GPT-3 se entrena previamente de manera autosupervisada explorando un poco m√°s <em>few-shots fine-tuning</em>.</p>
<a class="bg-primary mb-1 reference internal image-reference" href="_images/GPT-1-2-3_architecture.png"><img alt="comic xkcd 2421" class="bg-primary mb-1 align-center" src="_images/GPT-1-2-3_architecture.png" style="width: 600px;" /></a>
<p>Figura 5. Arquitecturas GPT. Fuente <a class="reference external" href="https://newsletter.theaiedge.io/p/the-chatgpt-models-family">https://newsletter.theaiedge.io/p/the-chatgpt-models-family</a></p>
<p>Adem√°s de estas versiones, tambi√©n existen variantes m√°s peque√±as de GPT para diferentes usos, como GPT-3 Lite y GPT-2 Medium. Cada una de estas variantes tiene un tama√±o y capacidad diferente, lo que las hace m√°s adecuadas para diferentes aplicaciones y escenarios.</p>
<section id="entrenamiento-del-gpt">
<h4>Entrenamiento del GPT<a class="headerlink" href="#entrenamiento-del-gpt" title="Permalink to this headline">#</a></h4>
<ul class="simple">
<li><p>GPT-1 est√° preentrenado en el conjunto de datos de BooksCorpus, que contiene ~7000 libros que suman ~5 GB de datos: <a class="reference external" href="https://huggingface.co/datasets/bookcorpus">https://huggingface.co/datasets/bookcorpus</a>.</p></li>
<li><p>GPT-2 se entrena previamente con el conjunto de datos de WebText, que es un conjunto m√°s diverso de datos de Internet que contiene ~8 millones de documentos para aproximadamente ~40 GB de datos: <a class="reference external" href="https://huggingface.co/datasets/openwebtext">https://huggingface.co/datasets/openwebtext</a></p></li>
<li><p>GPT-3 utiliza una versi√≥n ampliada del conjunto de datos de WebText, dos corpus de libros basados en Internet que no se divulgan y la Wikipedia en ingl√©s que constituy√≥ ~600 GB de datos.</p></li>
</ul>
<p>La implementaci√≥n de GPT-2 se puede encontrar en los siguientes repositorios:</p>
<ul class="simple">
<li><p>TensorFlow por OpenAI: <a class="reference external" href="https://github.com/openai/gpt-2/blob/master/src/model.py">https://github.com/openai/gpt-2/blob/master/src/model.py</a></p></li>
<li><p>PyTorch por Andrej Karpathy: <a class="reference external" href="https://github.com/karpathy/minGPT/blob/master/mingpt/model.py">https://github.com/karpathy/minGPT/blob/master/mingpt/model.py</a></p></li>
</ul>
<p>A continuaci√≥n se muestra un ejemplo de uso de GPT2 en un Pipeline:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">pipeline</span><span class="p">,</span> <span class="n">set_seed</span>
<span class="n">generator</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="s1">&#39;text-generation&#39;</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="s1">&#39;gpt2&#39;</span><span class="p">)</span>
<span class="n">set_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">generator</span><span class="p">(</span><span class="s2">&quot;My name is&quot;</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">num_return_sequences</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

</pre></div>
</div>
<p>GPT-3 API se encuentra disponible en el siguiente enlace: <a class="reference external" href="https://platform.openai.com/docs/introduction/overview">https://platform.openai.com/docs/introduction/overview</a></p>
</section>
<section id="ventajas">
<h4>Ventajas<a class="headerlink" href="#ventajas" title="Permalink to this headline">#</a></h4>
<ul class="simple">
<li><p><strong>Alto rendimiento en tareas de lenguaje natural:</strong> GPT est√° entrenado en una gran cantidad de texto en internet, lo que le permite desarrollar una comprensi√≥n profunda del lenguaje natural y su uso en diferentes contextos. Esto hace que mejore la capacidad de rendimiento y calidad en tareas como la traducci√≥n autom√°tica, la generaci√≥n de texto y la respuesta a preguntas.</p></li>
<li><p><strong>Facilidad de uso:</strong> GPT es un modelo pre-entrenado, lo que significa que no es necesario entrenarlo desde cero para cada tarea espec√≠fica. Esto significa que es m√°s f√°cil de usar para los desarrolladores y requiere menos recursos de hardware y tiempo de entrenamiento.</p></li>
<li><p><strong>Adaptabilidad:</strong> GPT puede ser finetuneado o adaptado a diferentes tareas y contextos espec√≠ficos. Esto permite que el modelo se ajuste a los requisitos espec√≠ficos de cada proyecto y mejore su rendimiento.</p></li>
<li><p><strong>Capacidad generativa:</strong> GPT es un modelo generativo, lo que significa que es capaz de generar texto de forma aut√≥noma. Esto es √∫til en una variedad de aplicaciones, como la generaci√≥n de contenido, la creaci√≥n de di√°logos virtuales y la respuesta a preguntas.</p></li>
</ul>
</section>
<section id="desventajas">
<h4>Desventajas<a class="headerlink" href="#desventajas" title="Permalink to this headline">#</a></h4>
<ul class="simple">
<li><p><strong>Bias y desigualdades:</strong> Al estar entrenado en una gran cantidad de texto en internet, GPT puede incorporar los sesgos y desigualdades presentes en la fuente de datos.</p></li>
<li><p><strong>Inseguridad:</strong> GPT es un modelo de aprendizaje autom√°tico, lo que significa que su rendimiento puede ser afectado por la calidad y la representatividad de la fuente de datos utilizada para su entrenamiento. Adem√°s, el modelo puede ser <strong>vulnerable a ataques y manipulaciones</strong>, como la generaci√≥n de texto falsificado o la respuesta a preguntas inapropiadas.</p></li>
<li><p><strong>Costos computacionales:</strong> GPT es un modelo grande y complejo que requiere una gran cantidad de recursos computacionales para su entrenamiento y uso. Esto puede resultar en costos elevados para el hardware y la energ√≠a, lo que puede ser un obst√°culo para algunos usuarios.</p></li>
<li><p><strong>Limitaciones en la comprensi√≥n del contexto:</strong> Aunque GPT ha sido entrenado en una gran cantidad de texto, todav√≠a puede tener dificultades para comprender el contexto en el que se utiliza el lenguaje natural. Esto puede resultar en respuestas poco precisas o inapropiadas en ciertos contextos.</p></li>
</ul>
</section>
<section id="decodificacion-de-gpt">
<h4>Decodificaci√≥n de GPT<a class="headerlink" href="#decodificacion-de-gpt" title="Permalink to this headline">#</a></h4>
<p>La decodificaci√≥n de GPT es el proceso de generar texto a partir de un modelo de lenguaje pre-entrenado.
Existen dos estrategias:</p>
<ul class="simple">
<li><p><strong>Determin√≠sticas</strong></p></li>
<li><p><strong>Estoc√°ticas</strong></p></li>
</ul>
<p><strong>En el siguiente material &lt;&lt;<a class="reference external" href="https://medium.com/&#64;shravankoninti/decoding-strategies-of-all-decoder-only-models-gpt-631faa4c449a">Decoding Strategies of all Decoder only Models (GPT)</a>&gt;&gt; se presenta un exaustiva expliaci√≥n de cada una de ellas. Dicho material es de obligatorio estudio.</strong></p>
<p><img alt="Alt text" src="_images/GPT_estrategias.png" />
Figura 6. Estrategias de decodificaci√≥n modelos GPT.</p>
<p>En el siguiente <a class="reference external" href="https://github.com/TeachingTextMining/TextClassification/blob/main/07-SA-Gen/decoderGPT.ipynb">cuaderno de trabajo</a> se utiliza GPT-2 para generar texto de diferentes alternativas de texto.</p>
<p>Se resumen a continuaci√≥n esas <strong>formas de decodificaci√≥n:</strong></p>
<ul class="simple">
<li><p><strong>Greedy Search</strong>: En el m√©todo de b√∫squeda greedy (avaricioso), se selecciona la palabra con la probabilidad m√°s alta en cada paso. Este m√©todo es r√°pido, pero puede generar texto que no es coherente o relevante. Este m√©todo elige la palabra con la probabilidad m√°s alta en cada paso.</p></li>
<li><p><strong>Beam Search</strong>: En el m√©todo de b√∫squeda de haz, se basa en la b√∫squeda avariciosa, pero en lugar de seleccionar la palabra con la probabilidad m√°s alta en cada paso, se seleccionan las k palabras con la probabilidad m√°s alta y se generan k secuencias parciales. Luego, se selecciona la secuencia parcial con la probabilidad m√°s alta en cada paso. Se define el par√°metro <code class="docutils literal notranslate"><span class="pre">num_beams</span></code> para controlar el n√∫mero de secuencias parciales generadas en cada paso. Adem√°s, este m√©todo permite definir par√°metros como <code class="docutils literal notranslate"><span class="pre">no_repeat_ngram_size</span></code> para evitar la repetici√≥n de n-gramas en el texto generado o <code class="docutils literal notranslate"><span class="pre">num_return_sequences</span></code> para controlar el n√∫mero de secuencias generadas.</p></li>
<li><p><strong>Sampling</strong>: El m√©todo de muestreo selecciona la palabra en cada paso de acuerdo con su probabilidad. Se define el par√°metro <code class="docutils literal notranslate"><span class="pre">temperature</span></code> para controlar la aleatoriedad en la selecci√≥n de palabras. Un valor m√°s alto de <code class="docutils literal notranslate"><span class="pre">temperature</span></code> aumenta la aleatoriedad y un valor m√°s bajo disminuye la aleatoriedad.</p></li>
<li><p><strong>Top-K Sampling</strong>: El m√©todo de muestreo Top-K selecciona las palabras en cada paso de acuerdo con su probabilidad, pero solo considera las k palabras con la probabilidad m√°s alta. Se define el par√°metro <code class="docutils literal notranslate"><span class="pre">top_k</span></code> para controlar el n√∫mero de palabras consideradas en cada paso. Este m√©todo, por tanto, elimina las palabras con baja probabilidad que suelen ser las que producen texto incoherente.</p></li>
<li><p><strong>Top-P Sampling</strong>: Mientras que el m√©todo de muestreo Top-K selecciona las palabras en cada paso de acuerdo con su probabilidad, pero solo considera las k palabras con la probabilidad m√°s alta, el m√©todo de muestreo Top-P selecciona las palabras en cada paso de acuerdo con su probabilidad, pero solo considera las palabras cuya probabilidad acumulada sea mayor que un cierto umbral p. Se define el par√°metro <code class="docutils literal notranslate"><span class="pre">top_p</span></code> para controlar el umbral de probabilidad acumulada.</p></li>
</ul>
<p>En el cuaderno referenciado al principio podr√°s ver ejemplos de generaci√≥n de texto con cada uno de estos m√©todos, para as√≠ ver los problemas de coherencia y repetici√≥n que pueden surgir con cada uno de ellos.</p>
</section>
</section>
<section id="copilot">
<h3>Copilot<a class="headerlink" href="#copilot" title="Permalink to this headline">#</a></h3>
<p>Es asistente de inteligencia artificial dise√±ado, por OpenAI, para ayudar enel completamiento de c√≥digo mediante el uso de la conversaci√≥n natural. Copilot utiliza modelos de lenguaje avanzados para comprender tus necesidades y brindarte la informaci√≥n y la ayuda que necesitas. Puedes interactuar con Copilot en una variedad de plataformas y dispositivos, incluyendo mensajer√≠a, aplicaciones de chat, aplicaciones de escritorio y m√°s. !Es <strong>orientado a servicios en la nube</strong>! Es decir, se se accede a los servicios online a trav√©s de una API.</p>
<p>Copilot est√° dise√±ado para ayudarte a realizar una amplia gama de tareas y responder preguntas de forma eficiente y precisa. Algunos ejemplos de las tareas que puedes realizar con Copilot incluyen:</p>
<ul class="simple">
<li><p><strong>Consultar informaci√≥n</strong> sobre el clima, la hora actual y otras condiciones meteorol√≥gicas.</p></li>
<li><p>Obtener informaci√≥n sobre <strong>eventos actuales, noticias y tendencias</strong>.</p></li>
<li><p>Realizar <strong>b√∫squedas en l√≠nea</strong> y encontrar informaci√≥n sobre temas espec√≠ficos.</p></li>
<li><p><strong>Programar recordatorios y citas</strong>.</p></li>
<li><p><strong>Obtener recomendaciones</strong> de restaurantes, pel√≠culas y otras formas de entretenimiento.</p></li>
<li><p><strong>Traducir</strong> palabras y frases a otros idiomas.</p></li>
<li><p><strong>Obtener informaci√≥n sobre la bolsa de valores</strong>, la tasa de cambio y otras cotizaciones financieras.</p></li>
<li><p><strong>Resolver problemas</strong> <strong>matem√°ticos</strong> y <strong>responder preguntas</strong> sobre <strong>conceptos cient√≠ficos</strong> y <strong>tecnol√≥gicos</strong>.</p></li>
</ul>
<p>Copilot est√° dise√±ado para ayudarte a realizar muchas tareas cotidianas y responder preguntas de una manera conveniente y r√°pida. Ejemplo de ello, lo podemos encontrar en la integraci√≥n de pluggins en <a class="reference external" href="https://docs.github.com/en/copilot/getting-started-with-github-copilot/getting-started-with-github-copilot-in-visual-studio-code">Visual Studio Code</a> para la completaci√≥n de c√≥digos.</p>
<section id="id1">
<h4>Ventajas<a class="headerlink" href="#id1" title="Permalink to this headline">#</a></h4>
<ul class="simple">
<li><p>Copilot utiliza una <strong>interfaz de conversaci√≥n natural</strong> (Visual y API) para interactuar con los usuarios, lo que hace que sea f√°cil y agradable de usar.</p></li>
<li><p>Est√° entrenado en una amplia gama de informaci√≥n y puede <strong>ayudar a los usuarios a encontrar y proporcionar informaci√≥n sobre una amplia variedad de temas</strong>.</p></li>
<li><p>Puede ayudar a los usuarios a realizar tareas y <strong>obtener informaci√≥n de manera m√°s r√°pida y eficiente</strong>, lo que les <strong>permite ser m√°s productivos</strong>.</p></li>
<li><p>Est√° <strong>dise√±ado para proporcionar una experiencia de usuario amigable y personalizada</strong>, lo que puede mejorar la satisfacci√≥n del usuario y fidelizaci√≥n.</p></li>
<li><p>Puede <strong>integrarse con otros servicios en l√≠nea</strong> para proporcionar una experiencia de usuario m√°s completa.</p></li>
</ul>
</section>
<section id="id2">
<h4>Desventajas<a class="headerlink" href="#id2" title="Permalink to this headline">#</a></h4>
<ul class="simple">
<li><p><strong>Costo:</strong> Copilot es un producto de OpenAI (empresa privada) y puede ser costoso pagar el uso de servicios para algunos usuarios, especialmente para aquellos que requieren una gran cantidad de uso o integraciones.</p></li>
<li><p><strong>Accesibilidad limitada:</strong> <strong>Solo est√° disponible como una API</strong>, por lo que solo puede ser utilizado por desarrolladores y no est√° disponible directamente para el p√∫blico en general.</p></li>
<li><p><strong>Capacidad limitada:</strong> Aunque Copilot est√° entrenado en una amplia gama de informaci√≥n, <strong>todav√≠a hay l√≠mites en su capacidad para comprender y responder</strong> a todas las preguntas y tareas.</p></li>
<li><p><strong>Confidencialidad y privacidad:</strong> Al usar Copilot, <strong>debes compartir tus datos y preocuparte por la privacidad y seguridad de ellos</strong>.</p></li>
<li><p><strong>Requiere habilidades t√©cnicas:</strong> Para <strong>integrar esta tecnolog√≠a</strong> en tus aplicaciones y servicios, <strong>debes tener habilidades</strong> t√©cnicas y conocimientos en programaci√≥n.</p></li>
</ul>
</section>
<section id="alternativas-a-copilot">
<h4>Alternativas a Copilot<a class="headerlink" href="#alternativas-a-copilot" title="Permalink to this headline">#</a></h4>
<ul class="simple">
<li><p><strong>Dialogflow</strong>: Una plataforma de Google que permite a los desarrolladores crear chatbots y aplicaciones de conversaci√≥n.</p></li>
<li><p><strong>IBM Watson Assistant</strong>: Una plataforma de inteligencia artificial de IBM que permite a los desarrolladores crear chatbots y aplicaciones de conversaci√≥n.</p></li>
<li><p><strong>Microsoft Bot Framework</strong>: Un marco de trabajo de Microsoft que permite a los desarrolladores crear chatbots y aplicaciones de conversaci√≥n para varias plataformas, incluidas las aplicaciones de mensajer√≠a, los sitios web y las aplicaciones de escritorio.</p></li>
<li><p><strong>Amazon Lex</strong>: Un servicio de Amazon Web Services que permite a los desarrolladores crear chatbots y aplicaciones de conversaci√≥n.</p></li>
<li><p><strong>Rasa</strong>: Un marco de software de c√≥digo abierto que permite a los desarrolladores crear chatbots y aplicaciones de conversaci√≥n.</p></li>
</ul>
</section>
</section>
<section id="chatgpt">
<h3>ChatGPT<a class="headerlink" href="#chatgpt" title="Permalink to this headline">#</a></h3>
<p>Es un modelo de lenguaje entrenado utilizando una gran cantidad de texto en internet. Se trata de una tecnolog√≠a de procesamiento del lenguaje natural que permite a los usuarios interactuar con el modelo mediante el uso de conversaciones naturales. !Es <strong>orientado a servicios en la nube</strong>! Es decir, se accede a los servicios online a trav√©s de una API.</p>
<p>Algunas de las funcionalidades m√°s destacadas incluyen:</p>
<ul class="simple">
<li><p><strong>Responder preguntas</strong>: ChatGPT puede responder preguntas sobre una amplia gama de temas, incluyendo <strong>historia, geograf√≠a, ciencias, tecnolog√≠a, programaci√≥n y mucho m√°s</strong>.</p></li>
<li><p><strong>Completar oraciones o fragmentos de texto</strong>: ChatGPT puede utilizar el contexto y la informaci√≥n previa para completar oraciones o fragmentos de texto de manera eficiente.</p></li>
<li><p><strong>Generar texto</strong>: ChatGPT puede generar texto en una variedad de formatos, como descripciones de productos, rese√±as de pel√≠culas y mucho m√°s.</p></li>
<li><p><strong>Traducci√≥n de idiomas</strong>: ChatGPT puede traducir palabras y frases a otros idiomas, lo que lo hace ideal para aquellos que desean comunicarse en un idioma distinto al suyo.</p></li>
<li><p><strong>Resumen de texto</strong>: ChatGPT puede resumir grandes cantidades de texto en una forma concisa y f√°cil de entender.</p></li>
<li><p><strong>An√°lisis de sentimientos</strong>: ChatGPT puede analizar el contenido de un texto para determinar el sentimiento que se expresa en √©l, como por ejemplo si es positivo, negativo o neutral.</p></li>
</ul>
<p>En la web oficial de OpenAI podemos ver un amplio listado de ejemplos de aplicaciones de esta tecnolog√≠a:</p>
<ul class="simple">
<li><p>Q&amp;A.</p></li>
<li><p>Correcci√≥n gramatical.</p></li>
<li><p>Resumir un texto.</p></li>
<li><p>Traducir un texto complejo en un simple concepto.</p></li>
<li><p>Llamadas a APIs para usar t√©cnicas de PLN.</p></li>
<li><p>Generar comandos de programaci√≥n a partir de instrucciones en lenguaje natural.</p></li>
<li><p>Traducci√≥n autom√°tica.</p></li>
<li><p>Generar codificaci√≥n de programaci√≥n: para llamar APIs, sentencias SQL, estructuras de programaci√≥n, etc., desde instrucciones en lenguaje natural.</p></li>
<li><p>Crear tabulaciones a partir de texto.</p></li>
<li><p>Separar contenido no estructurado.</p></li>
<li><p>Tareas de clasificaci√≥n.</p>
<ul>
<li><p>Extracci√≥n de categor√≠as impl√≠citas en textos.</p></li>
</ul>
</li>
<li><p>Generar descripciones y explicaciones a partir de c√≥digos Python.</p></li>
<li><p>Convertir el t√≠tulo de una pel√≠cula en un emoji.</p></li>
<li><p>Hallar la complejidad computacional de una funci√≥n.</p></li>
<li><p>Traducir de un lenguaje de programaci√≥n a otro.</p></li>
<li><p>Detecci√≥n de sentimientos para un fragmento de texto.</p></li>
<li><p>Explicar una pieza complicada de c√≥digo.</p></li>
<li><p>Extraer palabras clave de un bloque de texto.</p></li>
<li><p>Convertir la descripci√≥n de un producto en un texto publicitario.</p></li>
<li><p>Generador de nombres de productos.</p></li>
<li><p>Solucionar de errores de Python.</p>
<ul>
<li><p>Encontrar y corregir errores en el c√≥digo fuente.</p></li>
</ul>
</li>
<li><p>Crear de hojas de c√°lculo.</p></li>
<li><p>Responder preguntas de JavaScript.</p></li>
<li><p>Responder preguntas sobre modelos de lenguaje.</p></li>
<li><p>Crear una lista de elementos para un tema determinado.</p></li>
<li><p>Extracci√≥n de informaci√≥n.</p></li>
<li><p>Crear  microhistorias.</p></li>
<li><p>Convertir texto en tercera persona.</p></li>
<li><p>Generar esquemas para un tema.</p></li>
<li><p>Conversaci√≥n abierta con un asistente de IA.</p></li>
</ul>
<p>A continuaci√≥n se muestra la evoluci√≥n de modelos hasta lo que hoy conocemos como ChatGPT:</p>
<a class="bg-primary mb-1 reference internal image-reference" href="_images/GPT-1-2-3_datasources.png"><img alt="comic xkcd 2421" class="bg-primary mb-1 align-center" src="_images/GPT-1-2-3_datasources.png" style="width: 600px;" /></a>
<p>Figura 7. Evoluci√≥n de GPT hasta llegar a ChatGPT. Fuente <a class="reference external" href="https://newsletter.theaiedge.io/p/the-chatgpt-models-family">https://newsletter.theaiedge.io/p/the-chatgpt-models-family</a></p>
<section id="ejemplo-de-uso-de-la-api-chatgpt">
<h4>Ejemplo de uso de la API ChatGPT:<a class="headerlink" href="#ejemplo-de-uso-de-la-api-chatgpt" title="Permalink to this headline">#</a></h4>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">openai</span>

<span class="c1"># Inicializar la API de OpenAI</span>
<span class="n">openai</span><span class="o">.</span><span class="n">api_key</span> <span class="o">=</span> <span class="s2">&quot;tu_api_key_aqui&quot;</span>

<span class="c1"># Hacer una pregunta a ChatGPT</span>
<span class="n">response</span> <span class="o">=</span> <span class="n">openai</span><span class="o">.</span><span class="n">Completion</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
    <span class="n">engine</span><span class="o">=</span><span class="s2">&quot;text-davinci-002&quot;</span><span class="p">,</span> <span class="c1"># asignamos el nombre del modelo a utilizar. Ejemplo: &quot;text-davinci-003&quot;, &quot;text-davinci-002&quot;, &quot;text-davinci-001&quot;, &quot;code-davinci-002&quot;, ...</span>
    <span class="n">prompt</span><span class="o">=</span><span class="s2">&quot;Qu√© es el sol?&quot;</span><span class="p">,</span> <span class="c1"># Entrada</span>
    <span class="n">max_tokens</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span> <span class="c1"># Dimensionalidad de la ventana </span>
    <span class="n">n</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">stop</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">temperature</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
<span class="p">)</span>
<span class="c1"># Imprimir la respuesta</span>
<span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="p">[</span><span class="s2">&quot;choices&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;text&quot;</span><span class="p">])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="s2">&quot;El sol es una estrella.&quot;</span>
</pre></div>
</div>
<p>N√≥tese que para poder utilizar esta librer√≠a se ha de emplear un servicio en la nube del cual se ha de requerir una clave de acceso. Las instruciones para conseguirlas las pod√©is encontrar en el siguiente enlace: <a class="reference external" href="https://platform.openai.com/account/api-keys">https://platform.openai.com/account/api-keys</a></p>
</section>
<section id="id3">
<h4>Ventajas<a class="headerlink" href="#id3" title="Permalink to this headline">#</a></h4>
<ul class="simple">
<li><p><strong>Gran capacidad de comprensi√≥n del lenguaje natural</strong>: ChatGPT ha sido entrenado en una amplia variedad de textos y ha desarrollado una comprensi√≥n profunda del lenguaje humano, lo que le permite responder de manera fluida y natural a las preguntas y comentarios de los usuarios.</p></li>
<li><p><strong>Personalizaci√≥n</strong>: Puede ser personalizado para diferentes aplicaciones y usos, lo que lo hace ideal para una amplia variedad de industrias.</p></li>
<li><p><strong>Alta disponibilidad</strong>: Est√° <strong>disponible en la nube</strong> y puede ser accedido desde cualquier lugar con una conexi√≥n a Internet, lo que lo hace muy accesible para los usuarios.</p></li>
<li><p><strong>Rapidez y eficiencia</strong>: Es <strong>capaz de procesar grandes cantidades de informaci√≥n</strong> en un tiempo muy corto, lo que lo hace ideal para aplicaciones en tiempo real.</p></li>
<li><p><strong>Mejora continua</strong>: Est√° en constante desarrollo y mejora por parte de OpenAI, lo que significa que sus capacidades y funciones contin√∫an mejorando con el tiempo.</p></li>
</ul>
</section>
<section id="id4">
<h4>Desventajas<a class="headerlink" href="#id4" title="Permalink to this headline">#</a></h4>
<ul class="simple">
<li><p><strong>Limitaciones en la comprensi√≥n del contexto</strong>: Aunque ChatGPT ha sido entrenado en una amplia variedad de textos, todav√≠a puede tener dificultades para comprender el contexto completo de una conversaci√≥n, especialmente en situaciones m√°s complejas.</p></li>
<li><p><strong>Responsabilidad por la precisi√≥n de la informaci√≥n</strong>: Puede proporcionar informaci√≥n que no sea precisa o que sea enga√±osa. Es responsabilidad del usuario verificar la informaci√≥n proporcionada por ChatGPT antes de tomar decisiones o acciones importantes.</p></li>
<li><p><strong>Requisitos de infraestructura</strong>: Para utilizar ChatGPT, es <strong>necesario tener acceso a la infraestructura</strong> y los recursos necesarios para conectarse y comunicarse con el modelo. Esto <strong>puede ser un obst√°culo para algunos usuarios que no cuenten con la infraestructura adecuada</strong>. <strong>Acceso a internet obligatorio!!!</strong></p></li>
<li><p><strong>Costo</strong>: Su uso <strong>puede requerir una inversi√≥n significativa en t√©rminos de costos de licenciamiento</strong> y recursos de infraestructura.</p></li>
</ul>
</section>
<section id="alternativas-a-chatgpt">
<h4>Alternativas a ChatGPT<a class="headerlink" href="#alternativas-a-chatgpt" title="Permalink to this headline">#</a></h4>
<p>Algunas alternativas son:</p>
<ul class="simple">
<li><p><span class="xref myst"><strong>PEER de Meta AI</strong></span>: un lenguaje entrenado para <strong>imitar el proceso de escritura</strong>. Est√° entrenado en los <a class="reference external" href="https://dumps.wikimedia.org/enwiki/">datos del historial de edici√≥n de Wikipedia</a>. Se especializa en predecir ediciones y explicar las razones de esas ediciones. Es capaz de citar y citar documentos de referencia para respaldar las afirmaciones que genera. Es un transformador de 11 billones de par√°metros con la arquitectura t√≠pica de codificador-decodificador, y est√° superando a GPT-3 en la tarea en la que se especializa.</p></li>
<li><p><a class="reference external" href="https://arxiv.org/pdf/2201.08239.pdf"><strong>LaMDA de Google AI</strong></a>: un modelo de lenguaje entrenado para <strong>aplicaciones de di√°logo</strong>. Est√° pre-entrenado en de ~3 billones de documentos y ~1 billones de di√°logos y ajustado en datos generados por humanos para mejorar la calidad, la seguridad y la veracidad del texto generado. Tambi√©n est√° ajustado para aprender a llamar a un sistema externo de recuperaci√≥n de informaci√≥n, como la B√∫squeda de Google, una calculadora y un traductor, lo que lo convierte en un candidato mucho m√°s fuerte para reemplazar la B√∫squeda de Google que ChatGPT. Es un decodificador de  135 billones par√°metros solo el transformer.</p></li>
<li><p><a class="reference external" href="https://arxiv.org/pdf/2204.02311.pdf"><strong>PaLM de Google AI</strong></a> - El m√°s grande de todos: ¬°540 billones de par√°metros! Con capacidades innovadoras en aritm√©tica y razonamiento de sentido com√∫n. Est√° entrenado en 780 mil millones de tokens provenientes de conversaciones en redes sociales multiling√ºes, p√°ginas web multiling√ºes filtradas, libros, repositorios de GitHub, Wikipedia multiling√ºe y noticias.</p></li>
<li><p><a class="reference external" href="https://huggingface.co/chat/"><strong>HuggingChat</strong></a> es una alternativa de c√≥digo abierto a ChatGPT, desarrollada por Hugging Face. A diferencia de ChatGPT, HuggingChat est√° disponible para todos y se basa en los mejores modelos de chat de la comunidad. Su modelo actual es <a class="reference external" href="https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1">mistralai/Mixtral-8x7B-Instruct-v0.1</a></p></li>
<li><p><a class="reference external" href="https://www.deepseek.com/"><strong>DeepSeek</strong></a> es un modelo de lenguaje grande (LLM) avanzado, dise√±ado para tareas de procesamiento de lenguaje natural con capacidades mejoradas en generaci√≥n de texto, razonamiento y programaci√≥n</p></li>
<li><p><a class="reference external" href="https://chat.mistral.ai/"><strong>Mistral</strong></a> Mistral es un modelo de lenguaje grande (LLM) optimizado para eficiencia y rendimiento, destacando por su arquitectura ligera y su capacidad para generar texto de alta calidad con menor consumo computacional.</p></li>
<li><p><a class="reference external" href="https://www.llama.com/"><strong>LLaMA (Large Language Model Meta AI)</strong></a> es una familia de modelos de lenguaje desarrollada por Meta, dise√±ada para ser eficiente y accesible, destacando en generaci√≥n de texto, razonamiento y tareas de procesamiento del lenguaje natural, con un enfoque en la investigaci√≥n y la escalabilidad.</p></li>
</ul>
</section>
</section>
<section id="metodos-para-optimizar-modelos-preentrenados">
<h3>M√©todos para Optimizar Modelos Preentrenados<a class="headerlink" href="#metodos-para-optimizar-modelos-preentrenados" title="Permalink to this headline">#</a></h3>
<p>La ingenier√≠a de prompts (<em>prompt engineering</em>), el ajuste de prompts (<em>prompt tuning</em>) y el ajuste fino (<em>fine-tuning</em>) son tres m√©todos distintos aplicados a modelos de lenguaje preentrenados (LLMs) para mejorar su rendimiento en dominios nuevos o tareas espec√≠ficas. Estos m√©todos no son mutuamente excluyentes y cada uno est√° orientado a un caso de uso particular.</p>
<p>Cada una de estas t√©cnicas ofrece un enfoque diferente para aprovechar las capacidades de los modelos preentrenados. La elecci√≥n entre ellas depende de las necesidades espec√≠ficas de la aplicaci√≥n, como la disponibilidad de recursos computacionales, el nivel de personalizaci√≥n requerido para el modelo y el grado de interacci√≥n deseado con los par√°metros de aprendizaje del modelo.</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>M√©todo</p></th>
<th class="head"><p>Demanda de Recursos</p></th>
<th class="head"><p>Requiere Entrenamiento</p></th>
<th class="head"><p>Ideal Para</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><em>Prompt engineering</em></p></td>
<td><p>Ninguna</p></td>
<td><p>No</p></td>
<td><p>Adaptaciones r√°pidas sin costo computacional.</p></td>
</tr>
<tr class="row-odd"><td><p><em>Prompt tuning</em></p></td>
<td><p>Baja</p></td>
<td><p>S√≠</p></td>
<td><p>Mantener la integridad del modelo en m√∫ltiples tareas.</p></td>
</tr>
<tr class="row-even"><td><p><em>Fine-tuning</em></p></td>
<td><p>Alta</p></td>
<td><p>S√≠</p></td>
<td><p>Tareas que requieren una personalizaci√≥n profunda del modelo.</p></td>
</tr>
</tbody>
</table>
<section id="ingenieria-de-prompts-prompt-engineering">
<h4>Ingenier√≠a de prompts (<em>Prompt Engineering</em>)<a class="headerlink" href="#ingenieria-de-prompts-prompt-engineering" title="Permalink to this headline">#</a></h4>
<p>La ingenier√≠a de prompts no implica ning√∫n tipo de entrenamiento o reentrenamiento. Se basa completamente en que el usuario dise√±e prompts espec√≠ficos para el modelo. Requiere un entendimiento detallado de las capacidades de procesamiento del modelo y aprovecha el conocimiento intr√≠nseco ya integrado en el modelo. La ingenier√≠a de prompts no requiere recursos computacionales, ya que se basa √∫nicamente en la formulaci√≥n estrat√©gica de entradas para obtener los resultados deseados.</p>
<p>Entre las t√©cnicas m√°s populares de ingenier√≠a de prompts tenemos las siguientes.</p>
<ul>
<li><p><strong>Zero-shot</strong>: En este tipo de <em>promts</em> no se especifica ning√∫n ejemplo, sino que directamente se le pide que realice algo. Por ejemplo, si estamos en la tarea de an√°lisis de sentimientos, un prompt de <em>zero-shot</em> podr√≠a ser</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>&quot;No me ha gustado la pel√≠cula Dune parte 2&quot;?. Sentimiento:
</pre></div>
</div>
</li>
<li><p><strong>Few-shot</strong>: En este caso, las plantillas s√≠ que incluyen alg√∫n ejemplo. Volviendo al ejemplo anterior, una plantilla <em>few-shot</em> podr√≠a ser:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>&quot;No me ha gustado la pel√≠cula Dune parte 2&quot;?. Sentimiento: negativo.
&quot;Me ha gustado la pel√≠cula Kung Fu Panda 4&quot;. Sentimiento:
</pre></div>
</div>
</li>
<li><p><strong>Chain-of-Thoughts</strong>: Es una t√©cnica que gu√≠a a los LLMs para que expliquen, paso a paso, c√≥mo resuelven un problema antes de proporcionar la respuesta final. Este enfoque mejora sus capacidades de razonamiento al incentivar al modelo a descomponer problemas complejos en una secuencia l√≥gica de pasos, imitando un proceso de pensamiento humano. Como resultado, los LLM pueden abordar de manera m√°s efectiva tareas que requieren razonamiento l√≥gico y soluciones en m√∫ltiples etapas, como preguntas de matem√°ticas o de sentido com√∫n. Por ejemplo, un enfoque b√°sico pero efectivo consiste en simplemente usar una plantilla como:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">P</span><span class="p">:</span> <span class="p">{</span><span class="n">pregunta</span><span class="p">}</span>
<span class="n">R</span><span class="p">:</span> <span class="n">Pensemos</span> <span class="n">paso</span> <span class="n">a</span> <span class="n">paso</span><span class="o">.</span>
</pre></div>
</div>
<p>Por supuesto, este enfoque no es mutuamente excluyente con los anteriores, as√≠ que podr√≠a mejorarse a partir de combinarlo con <em>few-shot</em>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">P</span><span class="p">:</span> <span class="n">Sumando</span> <span class="n">los</span> <span class="n">siguientes</span> <span class="n">n√∫meros</span> <span class="n">obtienes</span> <span class="n">un</span> <span class="n">resultado</span> <span class="n">de</span> <span class="mi">3</span> <span class="n">cifras</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">23</span><span class="p">,</span> <span class="mf">40.</span>
<span class="n">R</span><span class="p">:</span> <span class="n">Pensamos</span> <span class="n">paso</span> <span class="n">a</span> <span class="n">paso</span><span class="o">.</span> <span class="n">Falso</span><span class="p">,</span> <span class="n">la</span> <span class="n">suma</span> <span class="n">de</span> <span class="mi">4</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">+</span> <span class="mi">6</span> <span class="o">+</span> <span class="mi">23</span> <span class="o">+</span> <span class="mi">40</span> <span class="o">=</span> <span class="mi">75</span><span class="p">,</span> <span class="n">de</span> <span class="n">dos</span> <span class="n">cifras</span><span class="o">.</span>
<span class="n">P</span><span class="p">:</span> <span class="n">Sumando</span> <span class="n">los</span> <span class="n">siguientes</span> <span class="n">n√∫meros</span> <span class="n">obtienes</span> <span class="n">un</span> <span class="n">resultado</span> <span class="n">de</span> <span class="mi">3</span> <span class="n">cifras</span><span class="p">:</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">40</span><span class="p">,</span> <span class="mf">55.</span>
<span class="n">R</span><span class="p">:</span> <span class="n">Pensemos</span> <span class="n">paso</span> <span class="n">a</span> <span class="n">paso</span><span class="o">.</span> <span class="n">Verdadero</span><span class="p">,</span> <span class="n">la</span> <span class="n">suma</span> <span class="n">de</span> <span class="mi">20</span> <span class="o">+</span> <span class="mi">30</span> <span class="o">+</span> <span class="mi">40</span> <span class="o">+</span> <span class="mi">55</span> <span class="o">=</span> <span class="mi">145</span><span class="p">,</span> <span class="n">de</span> <span class="mi">3</span> <span class="n">cifras</span><span class="o">.</span>
<span class="n">P</span><span class="p">:</span> <span class="p">{</span><span class="n">pregunta</span><span class="p">}</span>
<span class="n">R</span><span class="p">:</span> <span class="n">Pensemos</span> <span class="n">paso</span> <span class="n">a</span> <span class="n">paso</span><span class="o">.</span>
</pre></div>
</div>
</li>
</ul>
<p>Estas t√©cnicas de ingenier√≠a de prompts tambi√©n se pueden emplear para construir prompts en vista a ser usadas en <em>prompt tuning</em> (ver siguiente secci√≥n). Estos casos se suele hablar de <em>zero-shot learning</em>, <em>few-shot learning</em> y <em>chain of thought learning</em>.</p>
<blockquote>
<div><p>En <a class="reference external" href="https://github.com/TeachingTextMining/TextClassification/blob/main/07-SA-Gen/shotTechn.ipynb">este cuaderno de trabajo</a> se ilustran estas tres t√©cnicas aplicadas al modelo Falcon 7B en su versi√≥n instruida.</p>
</div></blockquote>
</section>
<section id="ajuste-de-prompts-prompt-tuning">
<h4>Ajuste de prompts (<em>Prompt Tuning</em>)<a class="headerlink" href="#ajuste-de-prompts-prompt-tuning" title="Permalink to this headline">#</a></h4>
<p>El ajuste de prompts modifica un conjunto de par√°metros adicionales, conocidos como ‚Äúprompts suaves‚Äù o ‚Äúsoft prompts,‚Äù que se integran en el procesamiento de entrada del modelo. Este m√©todo ajusta c√≥mo el modelo interpreta las entradas sin modificar por completo sus pesos, ofreciendo un equilibrio entre mejora del rendimiento y eficiencia de recursos. Es particularmente valioso cuando los recursos computacionales son limitados o cuando se requiere flexibilidad para m√∫ltiples tareas, ya que los pesos originales del modelo permanecen sin cambios tras aplicar esta t√©cnica.</p>
<p><img alt="Prompt Tuning" src="_images/prompt-tuning.png" />
Figura 8. Prompt Tuning (<a class="reference external" href="https://arxiv.org/abs/2104.08691">fuente</a>).</p>
<p>Para m√°s detalles sobre c√≥mo aplicar esta t√©cnica usando la librer√≠a de transformers, consultar este <a class="reference external" href="https://huggingface.co/docs/peft/en/package_reference/prompt_tuning">enlace</a>.</p>
<blockquote>
<div><p>En <a class="reference external" href="https://github.com/TeachingTextMining/TextClassification/blob/main/07-SA-Gen/promptTuning.ipynb">este cuaderno de trabajo</a> se muestra como realizar un prompt tuning sobre un gran modelo de lenguaje (LLM), concretamente, al modelo FLAN-T5 creado por Google..</p>
</div></blockquote>
<section id="diferencia-entre-ajuste-de-prompts-y-ajuste-fino">
<h5>Diferencia entre ajuste de prompts y ajuste fino<a class="headerlink" href="#diferencia-entre-ajuste-de-prompts-y-ajuste-fino" title="Permalink to this headline">#</a></h5>
<p>La diferencia principal entre el ajuste de prompts y el ajuste fino (que se explicar√° en detalle en la siguiente secci√≥n) radica tanto en el alcance de cada t√©cnica como en su prop√≥sito.</p>
<p>El ajuste fino permite una personalizaci√≥n profunda del modelo, modificando directamente todos los pesos de su estructura. Es ideal para aplicaciones espec√≠ficas donde se necesita maximizar el rendimiento para una tarea concreta. Por otro lado, el ajuste de prompts adopta un enfoque m√°s √°gil al centrarse √∫nicamente en aprender una representaci√≥n m√°s inteligente de la entrada, sin tocar los pesos del modelo. Esto lo convierte en una opci√≥n m√°s liviana y flexible.</p>
<p>Si bien es cierto que el ajuste fino puede optimizarse con t√©cnicas modernas como LoRA (que se abordar√° m√°s adelante), conceptualmente estas estrategias representan m√©todos distintos. El ajuste de prompts no busca alterar el modelo en su totalidad, sino mejorar la forma en que interpreta las entradas.</p>
<blockquote>
<div><p>En <a class="reference external" href="https://github.com/TeachingTextMining/TextClassification/blob/main/07-SA-Gen/instructionTuning.ipynb">este cuaderno de trabajo</a> se muestra como realizar un instruction tunning a un LLM, concretamente, al modelo <a class="reference external" href="https://huggingface.co/docs/transformers/model_doc/flan-t5">FLAN-T5</a> creado por Google.</p>
</div></blockquote>
</section>
</section>
<section id="ajuste-fino-fine-tuning">
<h4>Ajuste fino (<em>Fine-tuning</em>)<a class="headerlink" href="#ajuste-fino-fine-tuning" title="Permalink to this headline">#</a></h4>
<p>El ajuste fino es el m√°s intensivo en recursos, ya que implica un reentrenamiento completo del modelo con un conjunto de datos espec√≠fico para un prop√≥sito particular. Esto ajusta los pesos del modelo preentrenado, optimiz√°ndolo para captar las sutilezas del conjunto de datos, pero requiere recursos computacionales sustanciales y aumenta el riesgo de sobreajuste. Muchos modelos de lenguaje como ChatGPT pasan por un ajuste fino despu√©s de su entrenamiento inicial gen√©rico en la tarea de predicci√≥n de la siguiente palabra. El ajuste fino ense√±a a estos modelos c√≥mo funcionar como asistentes digitales, haci√©ndolos significativamente m√°s √∫tiles que un modelo entrenado de manera general.</p>
<section id="continual-pre-training">
<h5>Continual pre-training<a class="headerlink" href="#continual-pre-training" title="Permalink to this headline">#</a></h5>
<p>El <em>continual pretraining</em> es una forma de ajuste fino, pero con distinciones en su contexto y aplicaci√≥n. A diferencia del ajuste fino tradicional, que adapta un modelo preentrenado a un conjunto de datos espec√≠fico para una tarea concreta, el <em>continual pretraining</em> implica continuar el entrenamiento de un modelo en nuevos datos despu√©s de su fase inicial de preentrenamiento, sin realizar un reentrenamiento completo desde cero.</p>
<p>El objetivo del <em>continual pretraining</em> no es necesariamente optimizar el modelo para una tarea espec√≠fica, sino actualizar su comprensi√≥n bas√°ndose en nueva informaci√≥n, preservando al mismo tiempo el conocimiento adquirido en el preentrenamiento original. Esto puede incluir datos generales o espec√≠ficos de un dominio y puede abarcar m√∫ltiples √°reas tem√°ticas.</p>
<p>Por ejemplo, mientras que un ajuste fino podr√≠a usar un corpus m√©dico para especializar un modelo en tareas m√©dicas, el <em>continual pretraining</em> puede emplearse para extender la capacidad del modelo a manejar un lenguaje nuevo o datos m√°s recientes o cambientes en un dominio sin enfocarse exclusivamente en una tarea concreta.</p>
<p><img alt="Continual pre-training" src="_images/continual.png" />
Figura 8. Continual pre-training (<a class="reference external" href="https://medium.com/&#64;gilinachum/llm-domain-adaptation-using-continued-pre-training-part-1-3-e3d10fcfdae1">fuente</a>).</p>
<blockquote>
<div><p>En <a class="reference external" href="https://github.com/TeachingTextMining/TextClassification/blob/main/07-SA-Gen/continual.ipynb">este cuaderno de trabajo</a> se muestra como realizar un continual pre-training sobre un gran modelo de lenguaje (LLM), concretamente el modelo GPT-2.</p>
</div></blockquote>
</section>
<section id="fine-tuning-lora">
<h5>Fine-tuning LoRA<a class="headerlink" href="#fine-tuning-lora" title="Permalink to this headline">#</a></h5>
<p>El ajuste fino utilizando LoRA (Low-Rank Adaptation) es una t√©cnica eficiente que permite adaptar modelos grandes a tareas espec√≠ficas con un costo computacional mucho menor que el ajuste fino tradicional. LoRA introduce modificaciones en las capas del modelo mediante matrices de baja dimensi√≥n, ajustando solo una peque√±a parte de los par√°metros del modelo preentrenado. Esto reduce significativamente la cantidad de recursos necesarios para el ajuste fino y minimiza el riesgo de sobreajuste.</p>
<p>En lugar de ajustar todos los pesos del modelo, LoRA inserta capas auxiliares que se entrenan para captar las caracter√≠sticas relevantes del nuevo conjunto de datos o tarea. Estas capas adicionales no afectan los par√°metros originales del modelo, permitiendo que este conserve su funcionalidad general y aumentando la flexibilidad para adaptarse a m√∫ltiples tareas con un solo modelo base.</p>
<p>Por ejemplo, LoRA es particularmente √∫til en escenarios donde se necesita adaptar un modelo a diferentes dominios o idiomas sin tener que duplicar el costo de almacenamiento o entrenamiento para cada adaptaci√≥n. Su enfoque modular y eficiente ha hecho que sea una elecci√≥n popular en la personalizaci√≥n de grandes modelos de lenguaje.</p>
<p>Para m√°s detalles sobre c√≥mo aplicar esta t√©cnica usando la librer√≠a de transformers, consultar este <a class="reference external" href="https://huggingface.co/docs/peft/main/en/package_reference/lora">enlace</a>.</p>
<p><img alt="Low-Rank Adaptation" src="_images/lora.jpg" />
Figura 9. Low-Rank Adaptation (<a class="reference external" href="https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms">fuente</a>).</p>
</section>
</section>
</section>
<section id="instruccion-de-modelos">
<h3>Instrucci√≥n de modelos<a class="headerlink" href="#instruccion-de-modelos" title="Permalink to this headline">#</a></h3>
<p>La t√©cnica de instrucci√≥n de modelos (Instruction-Tuning, IT) es crucial para mejorar las capacidades y la controlabilidad de los modelos de lenguaje grandes (LLMs).</p>
<ul class="simple">
<li><p><strong>Metodolog√≠a general de IT</strong>: El ajuste de instrucciones implica entrenar a√∫n m√°s los LLMs utilizando pares de datos de (instrucci√≥n, salida). Estos pares consisten en instrucciones humanas y las salidas generadas por el modelo. El objetivo es cerrar la brecha entre la predicci√≥n de la siguiente palabra por parte de los LLMs y el objetivo de los usuarios de que los LLMs sigan instrucciones humanas.</p></li>
<li><p><strong>Construcci√≥n de conjuntos de datos de IT</strong>: Se crean los conjuntos de datos para el ajuste de instrucciones. Estos conjuntos contienen ejemplos de instrucciones junto con las salidas esperadas.</p></li>
<li><p><strong>Entrenamiento de modelos de IT</strong>: Son t√©cnicas de entrenamiento espec√≠ficas utilizadas para ajustar los LLMs seg√∫n las instrucciones proporcionadas.</p></li>
<li><p><strong>Aplicaciones en diferentes modalidades y dominios</strong>: El ajuste de instrucciones se aplica a diversas √°reas, como texto, im√°genes y otros tipos de datos.</p></li>
<li><p><strong>Factores que influyen en los resultados de IT</strong>: El tama√±o del conjunto de datos de instrucciones y la generaci√≥n de salidas de instrucciones son algunos de los factores que afectan los resultados del ajuste de instrucciones.</p></li>
</ul>
<p><img alt="Alt text" src="_images/GPT_instrucciones.png" />
Figura 10. Arquitectura de instrucciones GPT.</p>
<p>En el siguiente art√≠culo <a class="reference external" href="https://arxiv.org/pdf/2308.10792.pdf">&lt;&lt;<em>Instruction Tuning for Large Language Models: A Survey</em>&gt;&gt;</a> se revisan las posibles dificultades del IT y las cr√≠ticas en su contra, adem√°s de se√±alar las deficiencias actuales de las estrategias existentes y sugerir posibles √°reas de investigaci√≥n futura.</p>
<p><img alt="Alt text" src="_images/GPT_IT_ejemplo.png" />
Figura 11. Ejemplo de instrucciones GPT.</p>
<section id="diferencia-entre-instruccion-de-modelos-y-ajuste-de-prompts">
<h4>Diferencia entre instrucci√≥n de modelos y ajuste de prompts<a class="headerlink" href="#diferencia-entre-instruccion-de-modelos-y-ajuste-de-prompts" title="Permalink to this headline">#</a></h4>
<p>El ajuste de prompts y la instrucci√≥n de modelos son enfoques utilizados para optimizar modelos de lenguaje, pero con objetivos y m√©todos distintos. Mientras que el ajuste de prompts busca mejorar el desempe√±o del modelo en tareas espec√≠ficas al optimizar √∫nicamente los vectores del prompt sin alterar los pesos del modelo base, la instrucci√≥n de modelos implica modificar los pesos del modelo mediante el entrenamiento con un gran conjunto de datos de instrucciones variadas, lo que ampl√≠a su capacidad de generalizaci√≥n a diversas tareas. Como se detalla en la tabla a continuaci√≥n, el ajuste de prompts es menos complejo computacionalmente y requiere pocos ejemplos enfocados, pero su generalizaci√≥n es limitada. Por otro lado, la instrucci√≥n de modelos, aunque m√°s costosa, habilita un mejor entendimiento y ejecuci√≥n de una amplia gama de instrucciones.</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Aspecto</p></th>
<th class="head"><p>Prompt Tuning</p></th>
<th class="head"><p>Instruction Tuning</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Objetivo</p></td>
<td><p>Mejorar tareas espec√≠ficas.</p></td>
<td><p>Mejorar la capacidad general de seguir instrucciones.</p></td>
</tr>
<tr class="row-odd"><td><p>Modificaci√≥n del modelo</p></td>
<td><p>No se alteran los pesos del modelo base.</p></td>
<td><p>Se ajustan los pesos del modelo.</p></td>
</tr>
<tr class="row-even"><td><p>Dataset requerido</p></td>
<td><p>Pocos ejemplos, enfocados en una tarea concreta.</p></td>
<td><p>Gran conjunto de datos diversos de instrucciones.</p></td>
</tr>
<tr class="row-odd"><td><p>Generalizaci√≥n</p></td>
<td><p>Limitada a la tarea para la cual se ajust√≥ el prompt.</p></td>
<td><p>Amplia, con capacidad para responder a m√°s tareas.</p></td>
</tr>
<tr class="row-even"><td><p>Complejidad computacional</p></td>
<td><p>Menor (ajuste de vectores del prompt).</p></td>
<td><p>Mayor (entrenamiento de pesos del modelo).</p></td>
</tr>
</tbody>
</table>
</section>
<section id="flan">
<h4>FLAN<a class="headerlink" href="#flan" title="Permalink to this headline">#</a></h4>
<p>FLAN (Fine-tuned LAnguage Net) es una ‚Äúarquitectura‚Äù para realizar un instruction-tuning a un modelo de lenguaje pre-entrenado. El objetivo es que usando un peque√±o n√∫mero de actualizaciones sobre un modelo pre-entrenado, el modelo resultante sea capaz de realizar tareas espec√≠ficas que antes no era capaz de realizar.</p>
<p>Para ese instruction-tuning, FLAN define una serie de plantillas, estando estas organizadas en las diferentes tareas para los que se quiere realizar el instruction-tuning (text classification, named entity recognition, etc.). Estas plantillas son utilizadas para generar ejemplos de entrenamiento a partir de los cuales se realiza el instruction-tuning.</p>
<p>En la imagen siguiente se muestran los datasets que se utilizan, teniendo FLAN para todos ellos definidas las plantillas para as√≠ generar los conjuntos de instrucciones. Ver√°s que todos los datasets est√°n agrupados en las diferentes tareas que se pretenden realizar.</p>
<p><img alt="alt text" src="_images/GPT_IT_datasets.png" />
Figura 12. Datasets de instrucciones.</p>
<p>Google tras proponer FLAN, realiz√≥ un reentrenamiento de T5, Figura 13, con FLAN, estando publicado este modelo en diferentes tama√±os en una <a class="reference external" href="https://huggingface.co/collections/google/flan-t5-release-65005c39e3201fff885e22fb">colecci√≥n de Hugging Face</a>. En el siguiente art√≠culo <a class="reference external" href="https://paperswithcode.com/method/t5">&lt;&lt;<em>Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</em>&gt;&gt;</a>, se exploran las t√©cnicas de aprendizaje por transferencia para NLP y se introducen un marco unificado que convierte todos los problemas de lenguaje basados en texto en un formato texto-a-texto.</p>
<p><img alt="alt text" src="_images/Transformer5.png" />
Figura 13. Text-to-Text Transfer Transformer(T5)</p>
</section>
<section id="cuaderno-de-ejemplo-de-instruction-tuning">
<h4>Cuaderno de ejemplo de instruction-tuning<a class="headerlink" href="#cuaderno-de-ejemplo-de-instruction-tuning" title="Permalink to this headline">#</a></h4>
<p>Hemos visto que un modelo se puede reentrenar con instrucciones. En <a class="reference external" href="https://github.com/TeachingTextMining/TextClassification/blob/main/07-SA-Gen/practicaFlan.ipynb">este <strong>cuaderno de trabajo</strong></a> podr√°s ver un ejemplo usando un peque√±o dataset con anotaciones de enfermedades.</p>
<p>Puedes ver que se realiza una plantilla y se realiza un instruction-tuning al modelo <a class="reference external" href="https://huggingface.co/docs/transformers/model_doc/t5">T5</a> en su versi√≥n peque√±a.</p>
<p>Revisa el cuaderno y date cuenta de las librer√≠as que usamos. Especialmente en lo importado de la librer√≠a <em>transformers</em>.</p>
<p>Este cuaderno puedes f√°cilmente adaptarlo para realizar un instruction-tuning usando otro dataset y/u otras plantillas.</p>
</section>
</section>
</section>
<section id="bibliografia">
<h2>Bibliograf√≠a<a class="headerlink" href="#bibliografia" title="Permalink to this headline">#</a></h2>
<p>[1] <a class="reference external" href="https://huggingface.co/">https://huggingface.co/</a></p>
<p>[2] <a class="reference external" href="https://openai.com/blog/chatgpt/">https://openai.com/blog/chatgpt/</a></p>
<p>[3] Zhang, Y., Sun, S., Galley, M., Chen, Y. C., Brockett, C., Gao, X., ‚Ä¶ &amp; Dolan, B. (2019). Dialogpt: Large-scale generative pre-training for conversational response generation. arXiv preprint arXiv:1911.00536.</p>
<p>[4] <a class="reference external" href="https://newsletter.theaiedge.io/p/the-chatgpt-models-family">https://newsletter.theaiedge.io/p/the-chatgpt-models-family</a></p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Universitat d'Alacant<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>