
(label_introduccion)=
Introducci√≥n. Miner√≠a de textos y procesamiento del lenguaje natural
====================================================================

En este bloque se abordan aspectos relacionados con la obtenci√≥n de textos, as√≠ como de su preprocesamiento y tratamiento con t√©cnicas b√°sicas de procesamiento del lenguaje natural. El profesor de este bloque es Miquel Espl√† Gomis. 

La *miner√≠a de textos* es el conjunto de t√©cnicas y herramientas desarrolladas para extraer informaci√≥n de grandes colecciones textuales, tanto informaci√≥n impl√≠cita como expl√≠cita.

Esa informaci√≥n est√° codificada en textos, es decir, en un idioma o lengua. La miner√≠a de textos necesita, por tanto, interpretar (en mayor o menor medida) los textos y a partir de esa interpretaci√≥n extraer la informaci√≥n. La disciplina computacional que idea y desarrolla sistemas para la interpretaci√≥n ling√º√≠stica de los textos es el procesamiento del lenguaje natural (PLN o NLP por sus siglas en ingl√©s: *Natural Language Processing*), disciplina h√≠brida entre la ling√º√≠stica y la computaci√≥n.

Este segundo bloque de la asignatura cubre dos aspectos principales: 
1. la obtenci√≥n de contenidos textuales de Internet, una de las principales fuentes de este timpo de informaic√≥n, y
2. los conceptos fundamentales del procesamiento del lenguaje natural.
Tambi√©n se comentar√°n herramientas y recursos √∫tiles para la miner√≠a de textos.

## Pr√°cticas a entregar para este bloque

Durante las dos sesiones de este bloque, vamos a realizar una serie de pr√°cticas que permitir√°n experimentar con algunos de los elementos incluidos en los contenidos te√≥ricos de la asignatura. En la primera sesi√≥n de este bloque, las pr√°cticas se centrar√°n en explorar diferentes estrategias para la obtenci√≥n de documentos de texto a partir de Internet, extracci√≥n de texto relevante de √©stos, y preprocesamiento del texto extraido. En la segunda sesi√≥n nos centraremos en explorar algunas de las estrategias estudiadas en el contenido te√≥rico, concretamente centradas en la obtenci√≥n de representaciones vectoriales del texto, y de c√≥mo se pueden usar para explotar la informaci√≥n descargada y almacenada.

El **plazo de entrega para el informe de pr√°cticas de este bloque acaba el 27 de marzo de 2025** a las 23.59 horas. Las pr√°cticas, excepto casos puntuales acordados con el profesor de este bloque, se han de hacer en parejas. Recuerda que hay un examen final de la asignatura, por lo que es muy recomendable que ambos miembros del equipo se impliquen de igual manera. La entrtega se realizar√° mediante el env√≠o de un √∫nico PDF final a trav√©s de una tutor√≠a de UACloud.

A la hora de corregir los informes de pr√°cticas, no se penalizar√° el uso de asistentes generativos de texto. El uso de ciertas herramientas de inteligencia artificial puede aumentar tus capacidades profesionales. Sin embargo, el abuso de su uso hasta el punto de despersonalizar tus entregas, desproveerlas de tu estilo personal, de tus propios razonamientos o de contenido relevante s√≠ afectar√° negativamente a tu nota. Todo esto suele ser bastante f√°cil de detectar, pero para casos dudosos el profesor podr√° realizar una peque√±a prueba con el estudiante en la que no se permita el uso de asistentes generativos de texto para comprobar la consistencia entre lo entregado y lo que el estudiante es capaz de producir por s√≠ mismo.

## Primera sesi√≥n (06/03/2025)

**<span style="font-size: 1.15em">Contenidos a preparar antes de la sesi√≥n del 06/03/2025</span>**

Las actividades a realizar antes de clase son:

- Lectura de la [entrada sobre web scpraping](https://link.springer.com/content/pdf/10.1007/978-3-319-32010-6_483.pdf?pdf=inline%20link) en la *Encyclopedia of Big Data*. Es un documento corto e introductorio que no te llevar√° m√°s de 30 minutos üïíÔ∏è de trabajo.
- Lectura de las p√°ginas de la 1 a la 12, y de la 169 a la 172 del libro [Website scraping with Python](https://link.springer.com/book/10.1007/978-1-4842-3925-4) de *G√°bor L√°szl√≥ Hajba*. En estas secciones se ampl√≠an los conceptos introducidos por el anterior documento, y se introduce una orientaci√≥n m√°s pr√°ctica a la tarea de scraping. Esta lectura te llevar√° aproximadamente 1 hora üïíÔ∏è de trabajo.
- Lectura del art√≠culo [*Comparison of text preprocessing methods*](https://www.cambridge.org/core/services/aop-cambridge-core/content/view/43A20821D65F1C0C4366B126FC794AE3/S1351324922000213a.pdf/comparison-of-text-preprocessing-methods.pdf), en el que se describen y eval√∫an diferentes estrategias de preprocesamiento de texto para tareas de PLN. Lee s√≥lo hasta la secci√≥n 3.5. Esta lectura te llevar√° alrededor de 1,5 horas üïíÔ∏è de trabajo.
- En el documento anterior se habla de la tokenizaci√≥n de texto a nivel de oraci√≥n, palabra y car√°cter, pero se omite un tipo de tokenizaci√≥n muy relevante a d√≠a de hoy: la tokenizaci√≥n a nivel de subpalabra. Por eso se sugiere tambi√©n la lectura del [apartado sobre tokenizaci√≥n](https://huggingface.co/docs/transformers/main/tokenizer_summary) del tutorial sobre Transformers de HuggingFace. Puedes desplazarte en el documento hasta el apartado que se centra en la tokenizaci√≥n a nivel de subpalabra y leer desde ese punto hasta el final para una introducci√≥n a las estrategias m√°s habituales. Esta lectura te llevar√° alrededor de 30 minutos üïíÔ∏è de trabajo.
- Lectura del art√≠culo [A review of the challenges with massive web-mined corpora used in large language models pre-training](https://arxiv.org/pdf/2407.07630). En este art√≠culo se hace un repaso de los retos principales referentes a la construcci√≥n de grandes corpus textuales a partir de contenidos recolectados de internet en uno de los escenarios m√°s habituales: el entrenamiento de grandes modelos de lengua. Esta lectura te llevar√° alrededor de 1 hora üïíÔ∏è de trabajo.

En total, todo el trabajo previo a la clase te llevar√° alrededor de 4 horas üïíÔ∏è. Ten en cuenta que algunos contenidos s√≥lo son accesibles de forma gratuita desde dentro de la red de la Universidad. Si los consultas desde casa, puedes utilizar la utilidad RED UA disponible en UACloud para acceder a ellos.

**Nota adicional**: Si la adquisici√≥n de textos de Internet te interesa y en el futuro quieres ampliar tus conocimientos, el libro [Web Scraping with Python](https://www.oreilly.com/library/view/web-scraping-with/9781098145347/) de Ryan Mitchell (actualmente en su tercera edici√≥n) me parece una guia exhaustiva y con un planteamiento muy pr√°ctico sobre las principales estrategias y retos para profundizar en esta tarea.

**<span style="font-size: 1.15em">Contenidos para la sesi√≥n presencial del 06/03/2025</span>**
- Realizaci√≥n del [tutorial de RealPython](https://realpython.com/beautiful-soup-web-scraper-python/#scrape-the-fake-python-job-site) sobre descarga de documentos est√°ticos de la web y extracci√≥n de texto.
- Ampliaci√≥n del tutorial anterior abordando la tarea de descarga con un un navegador tipo "headless", como [Puppeteer](https://pptr.dev/guides/getting-started), [Selenium](https://www.selenium.dev/documentation/webdriver/getting_started/first_script/) o [requests-HTML](https://requests-html.kennethreitz.org/).
- Comparaci√≥n de al menos dos herramientas para extraer texto de documentos PDF. Pod√©is probar herramientas como [PyMuPDF](https://pymupdf.readthedocs.io/en/latest/tutorial.html), [PyPDF](https://pypdf.readthedocs.io/en/stable/user/extract-text.html), [PDFMiner](https://pdfminersix.readthedocs.io/en/latest/), o [PDFtoText](https://pypi.org/project/pdftotext/). Probad las herramientas sobre los PDFs de los art√≠culos: [Using Machine Translation to Provide Target-Language Edit Hints in Computer Aided Translation Based on Translation Memories](https://www.dlsi.ua.es/~fsanchez/pub/pdf/espla-gomis15a.pdf) y [MaCoCu: Massive collection and curation of monolingual and bilingual data: focus on under-resourced languages](https://aclanthology.org/2022.eamt-1.41.pdf). Comentad las diferencias y problemas que detect√°is.
- Comparaci√≥n de herramientas para la extracci√≥n del texto principal de p√°ginas web. Pod√©is probar diferentes herramientas como [Trafilatura](https://trafilatura.readthedocs.io/en/latest/quickstart.html) , [readability](https://pypi.org/project/readability/), o  [BoilerPy3](https://pypi.org/project/boilerpy3/). Haced pruebas con diferentes p√°ginas web, una sugerencia pueden ser p√°ginas que contengan not√≠cias de medios digitales, que suelen tener mucho contenido descartable.
- **Ejercicio final de la sesi√≥n**: V√°is a recolectar datos de las publicaciones de la revista de la [*Sociedad Espa√±ola para el Procesamiento del Lenguaje Natural*](http://journal.sepln.org). Para esto, acceder√≠eis al [archivo de publicaciones de la revista](http://journal.sepln.org/sepln/ojs/ojs/index.php/pln/issue/archive) y acceder√©is a los s√©is n√∫meros publicados entre 2016 y 2018. Deb√©is implementar una herramienta que haga uso de una librer√≠a de scrapping para recolectar, para c√°da art√≠culo en estos n√∫meros de la resvista, la siguiente informaci√≥n: el t√≠tulo, el abstract, el a√±o de publicaci√≥n y la URL correspondiente al art√≠culo. La informaci√≥n recolectada se guardar√° en un fichero con [formato JSON](https://docs.python.org/3/library/json.html); para cada art√≠culo se registrar√°n los campos `title`, `abstract`, `url` y `year`, como en el siguiente ejemplo:

```json
[{"title": "Rule Extraction for Tree-to-Tree Transducers by Cost Minimization", "abstract": "Finite-state transducers give efficient representations of many Natural Language phenomena. They allow to account for complex lexicon restrictions encountered, without involving the use of a large set of complex rules difficult to analyze. We here show that these representations can be made very compact, indicate how to perform the corresponding minimization, and point out interesting linguistic side-effects of this operation.", "url": "http://aclweb.org/anthology/D16-1002", "year": "2016"},...]
```

## Segunda sesi√≥n (13/03/2025)

**<span style="font-size: 1.15em">Contenidos a preparar antes de la sesi√≥n del 13/03/2025</span>**

Las actividades a realizar antes de clase son:

- Ver el v√≠deo de la clase magistral de Yulia Tsvetkov sobre [an√°lisis morfol√≥gico y flexi√≥n morfol√≥gica](https://www.youtube.com/watch?v=y9sVFrmGu0w). La duraci√≥n de la clase es de 45 minutos üïíÔ∏è.
- Ver v√≠deo de la clase magistral de Graham Neubig sobre [an√°lisis de dependencias sint√°cticas](https://www.youtube.com/watch?v=y9sVFrmGu0w). Aunque el v√≠deo es m√°s largo, s√≥lo necesito que ve√°is la primera parte de la sesi√≥n, que acaba alrededor del minuto 38 üïíÔ∏è.
- Haz el tutorial sobre la herramienta de [procesamiento morfo-sint√°ctico Stanza](https://applied-language-technology.mooc.fi/html/notebooks/part_iii/01_multilingual_nlp.html) publicado por el grupo *Applied Language Technology* de la Universidad de Helsinki. El tutorial te llevar√° entre 1,25 y 1,5 horas üïíÔ∏è.
- Lectura del cap√≠tulo 6 del libro [*Speech and Language Processing*](https://web.stanford.edu/~jurafsky/slp3/6.pdf) de Daniel Jurafsky y James H. Martin (2024). S√≥lo necesitas leer hasta la secci√≥n 6.5, ya que los contenidos de secciones posteriores ya han sido cubiertas. Esta lectura te llevar√° alrededor de 1 hora üïíÔ∏è.

En total, todo el trabajo previo a la clase te llevar√° alrededor de 4 horas üïíÔ∏è.


