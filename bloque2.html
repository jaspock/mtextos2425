
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>18. T√©cnicas para la miner√≠a de textos &#8212; Miner√≠a de Textos</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/estilos.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="19. Uso de Google Colab con una VM personalizada" href="doc_utils/creditosColab/web.html" />
    <link rel="prev" title="17. Ev. Evaluaci√≥n de pr√°cticas del Bloque 2" href="bloque3_ev.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="_static/logo-master-ca.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Miner√≠a de Textos</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Materiales de Miner√≠a de Textos
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Bloque 1
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="bloque1.html">
   1. Introducci√≥n. Miner√≠a de textos y procesamiento del lenguaje natural
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bloque1_Practica1.html">
   2. Pr√°ctica 1a: PLN con SpaCy
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bloque1_Practica2.html">
   3. Pr√°ctica 1b :
   <em>
    Topic modeling
   </em>
   .
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Bloque 2
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="bloque3.html">
   4. Aplicaciones de la miner√≠a de textos
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bloque3_t1_aplicaciones.html">
   5. T1. Aplicaciones generales
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bloque3_t2_subaplicaciones-benchmarks.html">
   6. T2. Aplicaciones espec√≠ficas y Benchmacks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bloque3_t2.1_analisis_sentimientos.html">
   7. T2.1. Aplicaciones espec√≠ficas. An√°lisis de Sentimientos
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bloque3_t3.1_metricas.html">
   8. T3. M√©tricas de Evaluaci√≥n
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bloque3_t4_centralizacion.html">
   9. T4. Centralizaci√≥n de datasets y modelos: Huggingface, OpenAI
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bloque3_t5_automl.html">
   10. T5. Auto Machine Learning(AutoML)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bloque3_t5.1_autogoal.html">
   11. T5.1. AutoGOAL
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bloque3_p1_SA-Pipeline-Reviews.html">
   12. P1.1. Pipeline simple
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bloque3_p2_SA-Transformers-Basic.html">
   13. P1.2. APIs Transformers
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bloque3_p3_SA-Transformers-Training-FineTuning.html">
   14. P2. Reajustar modelos Transformers
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bloque3_p4_SA-Transformers-Training-Custom.html">
   15. P3. Composici√≥n de vectores de caracter√≠sticas
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bloque3_p5-SA-Ensemble.html">
   16. P4. Ensemble de pipelines
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bloque3_ev.html">
   17. Ev. Evaluaci√≥n de pr√°cticas del Bloque 2
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Bloque 3
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   18. T√©cnicas para la miner√≠a de textos
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Utilidades
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="doc_utils/creditosColab/web.html">
   19. Uso de Google Colab con una VM personalizada
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="_sources/bloque2.md"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.md</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#practicas-a-entregar-para-este-bloque">
   18.1. Pr√°cticas a entregar para este bloque
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#primera-sesion-25-de-abril-de-2024">
   18.2. Primera sesi√≥n (25 de abril de 2024)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#segunda-sesion-2-de-mayo-de-2024">
   18.3. Segunda sesi√≥n (2 de mayo de 2024)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#tercera-sesion-9-de-mayo-de-2024">
   18.4. Tercera sesi√≥n (9 de mayo de 2024)
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>T√©cnicas para la miner√≠a de textos</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#practicas-a-entregar-para-este-bloque">
   18.1. Pr√°cticas a entregar para este bloque
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#primera-sesion-25-de-abril-de-2024">
   18.2. Primera sesi√≥n (25 de abril de 2024)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#segunda-sesion-2-de-mayo-de-2024">
   18.3. Segunda sesi√≥n (2 de mayo de 2024)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#tercera-sesion-9-de-mayo-de-2024">
   18.4. Tercera sesi√≥n (9 de mayo de 2024)
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="tecnicas-para-la-mineria-de-textos">
<span id="label-tecnicas"></span><h1><span class="section-number">18. </span>T√©cnicas para la miner√≠a de textos<a class="headerlink" href="#tecnicas-para-la-mineria-de-textos" title="Permalink to this headline">#</a></h1>
<p>En este bloque se aborda el estudio de algunos modelos neuronales utilizados para procesar textos. El profesor de este bloque es Juan Antonio P√©rez Ortiz.</p>
<p>El bloque comienza con un repaso del funcionamiento del regresor log√≠stico, que nos servir√° para asentar los conocimientos necesarios para entender posteriores modelos. A continuaci√≥n se estudia con cierto nivel de detalle <em>skip-grams</em>, uno de los algoritmos para la obtenci√≥n de <em>embeddings</em> incontextuales de palabras. Despu√©s se repasa el funcionamiento de las arquitecturas neuronales <em>feedforward</em> y se estudia su aplicaci√≥n a modelos de lengua. El objetivo √∫ltimo es abordar el estudio de la arquitectura m√°s importante de los sistemas actuales de procesamiento de textos: el transformer. Una vez estudiadas estas arquitecturas, finalizaremos con un an√°lisis del funcionamiento de los modelos preentrenados (modelos fundacionales), en general, y de los modelos de lengua, en particular.</p>
<p>Los materiales de clase complementan la lectura de algunos cap√≠tulos de un libro de texto (‚ÄúSpeech and Language Processing‚Äù de Dan Jurafsky y James H. Martin, borrador de la tercera edici√≥n, disponible online) con anotaciones realizadas por el profesor.</p>
<section id="practicas-a-entregar-para-este-bloque">
<h2><span class="section-number">18.1. </span>Pr√°cticas a entregar para este bloque<a class="headerlink" href="#practicas-a-entregar-para-este-bloque" title="Permalink to this headline">#</a></h2>
<p>Durante las sesiones de este bloque, estudiaremos diferentes implementaciones en PyTorch de modelos neuronales para procesar textos. Para cada cuaderno de c√≥digo se plantear√° uno o m√°s ejercicios que tendr√°s que resolver y entregar en forma de un informe final. <strong>Importante:</strong> solo has de realizar los ejercicios que se indican en esta p√°gina de la asignatura y no los que pudieran aparecer al final del cuaderno. La longitud m√°xima de la respuesta de cada ejercicio es de 800 palabras, excluyendo los fragmentos de c√≥digo. En cualquier caso, mant√©n estos fragmentos de c√≥digo al m√≠nimo y no copies todo el c√≥digo de los cuadernos, sino solo aquellas l√≠neas m√°s relevantes para complementar tu respuesta. Entrega un √∫nico PDF final a trav√©s de una tutor√≠a de UACloud. El <strong>plazo de entrega acaba el 27 de mayo de 2024</strong> a las 23.59 horas. Las pr√°cticas se pueden hacer en parejas. Recuerda que hay un examen final de la asignatura, por lo que es muy recomendable que ambos miembros del equipo se impliquen de igual manera.</p>
<p>A la hora de corregir los informes de pr√°cticas, no se penalizar√° el uso de asistentes generativos de texto. M√°s bien al contrario: el uso de ciertas herramientas de inteligencia artificial puede aumentar tus capacidades profesionales. Sin embargo, el abuso de su uso hasta el punto de despersonalizar tus entregas, desproveerlas de tu estilo personal, de tus propios razonamientos o de contenido relevante s√≠ afectar√° negativamente a tu nota. Todo esto suele ser bastante f√°cil de detectar, pero para casos dudosos el profesor podr√° realizar una peque√±a prueba con el estudiante en la que no se permita el uso de asistentes generativos de texto para comprobar la consistencia entre lo entregado y lo que el estudiante es capaz de producir por s√≠ mismo.</p>
</section>
<section id="primera-sesion-25-de-abril-de-2024">
<h2><span class="section-number">18.2. </span>Primera sesi√≥n (25 de abril de 2024)<a class="headerlink" href="#primera-sesion-25-de-abril-de-2024" title="Permalink to this headline">#</a></h2>
<p><strong><span style="font-size: 1.15em">Contenidos a preparar antes de la sesi√≥n del 25/04/2024</span></strong></p>
<p>Las actividades a realizar antes de esta clase son:</p>
<ul class="simple">
<li><p>Lectura y estudio de los contenidos de <a class="reference external" href="https://www.dlsi.ua.es/~japerez/materials/transformers/regresor">esta p√°gina</a> sobre regresi√≥n log√≠stica. Como ver√°s, la p√°gina te indica qu√© contenidos has de leer del libro. Tras una primera lectura, lee las anotaciones del profesor, cuyo prop√≥sito es ayudarte a entender los conceptos clave del cap√≠tulo. Despu√©s, realiza una segunda lectura del cap√≠tulo del libro. En total, esta parte deber√≠a llevarte unas 3 horas üïíÔ∏è de trabajo.</p></li>
<li><p>Visionado y estudio de los tutoriales en v√≠deo de esta <a class="reference external" href="https://www.youtube.com/playlist?list=PL_lsbAsL_o2CTlGHgMxNrKhzP97BaG9ZN">playlist oficial de PyTorch</a>.  Estudia al menos los 4 primeros v√≠deos (‚ÄúIntroduction to PyTorch‚Äù, ‚ÄúIntroduction to PyTorch Tensors‚Äù, ‚ÄúThe Fundamentals of Autograd‚Äù y ‚ÄúBuilding Models with PyTorch‚Äù). En total, esta parte deber√≠a llevarte unas 2 horas üïíÔ∏è de trabajo.</p></li>
<li><p>Tras acabar con las dos partes anteriores, realiza este <a class="reference external" href="https://forms.gle/E1xzZHw6hzMWJaNr7">test de evaluaci√≥n</a> de estos contenidos. Son pocas preguntas y te llevar√° unos minutos.</p></li>
</ul>
<p><strong><span style="font-size: 1.15em">Contenidos para la sesi√≥n presencial del 25/04/2024</span></strong></p>
<p>En la clase presencial (3 horas üïíÔ∏è de duraci√≥n), repasaremos los contenidos de la semana anterior y veremos c√≥mo se implementa un regresor log√≠stico en PyTorch siguiendo la implementaci√≥n de un regresor log√≠stico binario y de uno multinomial que se comentan en <a class="reference external" href="https://www.dlsi.ua.es/~japerez/materials/transformers/implementacion/#codigo-para-un-regresor-logistico-y-uno-multinomial">este apartado</a>.</p>
<p><strong>Ejercicios</strong>: para este bloque, haz los siguientes ejercicios. Repasa las normas que se indican m√°s arriba sobre c√≥mo entregar los ejercicios.</p>
<ol class="simple">
<li><p><strong>Regresor log√≠stico binario</strong>: modifica el c√≥digo para que el conjunto de entrenamiento se divida en en entrenamiento, validaci√≥n y test. Usa el conjunto de evaluaci√≥n para determinar cu√°ndo detener el entrenamiento. A√±ade un gr√°fico que muestre la evoluci√≥n de la funci√≥n de p√©rdida en el conjunto de entrenamiento y en el de validaci√≥n.</p></li>
<li><p><strong>Regresor log√≠stico multinomial</strong>: estudia las probabilidades emitidas por el modelo ya entrenado para cada clase tanto con las frases del cuaderno como con algunas frases nuevas que t√∫ propongas. Juega con frases que est√©n a medio camino entre diferentes tem√°ticas.</p></li>
</ol>
</section>
<section id="segunda-sesion-2-de-mayo-de-2024">
<h2><span class="section-number">18.3. </span>Segunda sesi√≥n (2 de mayo de 2024)<a class="headerlink" href="#segunda-sesion-2-de-mayo-de-2024" title="Permalink to this headline">#</a></h2>
<p><strong><span style="font-size: 1.15em">Contenidos a preparar antes de la sesi√≥n del 02/05/2024</span></strong></p>
<p>Las actividades a realizar antes de esta clase son:</p>
<ul class="simple">
<li><p>Lectura y estudio de <a class="reference external" href="https://www.dlsi.ua.es/~japerez/materials/transformers/embeddings">esta p√°gina</a> sobre la obtenci√≥n de embeddings incontextuales. Como ver√°s, la p√°gina te indica qu√© contenidos has de leer del libro. Tras una primera lectura, lee las anotaciones del profesor, cuyo objetivo es ayudarte a entender los conceptos clave del cap√≠tulo. Despu√©s, realiza una segunda lectura del cap√≠tulo. En total, esta parte deber√≠a llevarte unas 3 horas üïíÔ∏è de trabajo.</p></li>
<li><p>Lectura y estudio de <a class="reference external" href="https://www.dlsi.ua.es/~japerez/materials/transformers/ffw">esta p√°gina</a> sobre las redes neuronales hacia delante. En total, esta parte deber√≠a llevarte unas 2 horas üïíÔ∏è de trabajo.</p></li>
<li><p>Primeros pasos en el estudio del modelo transformer. Volveremos a dedicar m√°s horas a esta arquitectura para la pr√≥xima sesi√≥n de forma que la abordaremos en dos fases. Por ahora, lee con detenimiento la introducci√≥n a mecanismos de atenci√≥n de <a class="reference external" href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/">‚ÄúVisualizing A Neural Machine Translation Model‚Äù</a>, as√≠ como la introducci√≥n visual a los transformers de <a class="reference external" href="http://jalammar.github.io/illustrated-transformer/">‚ÄúThe Illustrated Transformer‚Äù</a> y la m√°s elaborada de <a class="reference external" href="https://jalammar.github.io/illustrated-gpt2/">‚ÄúThe Illustrated GPT-2‚Äù</a>. A continuaci√≥n, lee el apartado 9.7 (solo este apartado) del cap√≠tulo <a class="reference external" href="https://web.archive.org/web/20221216193204/https://web.stanford.edu/~jurafsky/slp3/9.pdf">‚ÄúDeep learning architectures for sequence processing‚Äù</a>; el objetivo es que entiendas conceptualmente el mecanismo de atenci√≥n de los transformers, pero no es necesario que en este momento comprendas todos los detalles t√©cnicos (especialmente las ecuaciones del modelo), ya que volver√°s a dedicarle tiempo a este cap√≠tulo m√°s adelante. En total, esta parte deber√≠a llevarte ahora unas 3 horas üïíÔ∏è de trabajo.</p></li>
<li><p>Realizaci√≥n del <a class="reference external" href="https://forms.gle/Eb3ZwwGxbQp88t4FA">test de evaluaci√≥n</a> de estos contenidos. Son pocas preguntas y te llevar√° unos minutos.</p></li>
</ul>
<p><strong><span style="font-size: 1.15em">Contenidos para la sesi√≥n presencial del 02/05/2024</span></strong></p>
<p>En la clase presencial (3 horas üïíÔ∏è de duraci√≥n), repasaremos los contenidos de la semana anterior y veremos sendas implementaciones en PyTorch del algoritmo <a class="reference external" href="https://www.dlsi.ua.es/~japerez/materials/transformers/implementacion/#codigo-para-skip-grams">skip-grams</a> y de un modelo de lengua basado en <a class="reference external" href="https://www.dlsi.ua.es/~japerez/materials/transformers/implementacion/#codigo-para-un-modelo-de-lengua-con-redes-feedforward">redes feedforward</a>.</p>
<p><strong>Ejercicios</strong>: para este bloque, haz los siguientes ejercicios. Repasa las normas que se indican m√°s arriba sobre c√≥mo entregar los ejercicios. Aunque ambos ejercicios se basan en el cuaderno del algoritmo skip-grams, ten en cuenta que el cuaderno de las redes feedforward tambi√©n te ser√° muy √∫til para preparar el examen.</p>
<ol class="simple">
<li><p><strong>Skip-grams</strong>: modifica el c√≥digo para que se pueda seleccionar el tama√±o de la ventana L y realiza un peque√±o estudio sobre c√≥mo esto afecta a los embeddings obtenidos.</p></li>
<li><p><strong>Skip-grams</strong>: sustituye la parte del c√≥digo que usa la notaci√≥n de Einstein por una multiplicaci√≥n convencional de matrices seguida de una operaci√≥n que se quede con los valores que nos interesan. Compara los tiempos de ejecuci√≥n de ambas implementaciones.</p></li>
</ol>
</section>
<section id="tercera-sesion-9-de-mayo-de-2024">
<h2><span class="section-number">18.4. </span>Tercera sesi√≥n (9 de mayo de 2024)<a class="headerlink" href="#tercera-sesion-9-de-mayo-de-2024" title="Permalink to this headline">#</a></h2>
<p><strong><span style="font-size: 1.15em">Contenidos a preparar antes de la sesi√≥n del 09/05/2023</span></strong></p>
<p>Las actividades a realizar antes de esta clase son:</p>
<ul class="simple">
<li><p>Afianzar el estudio de <a class="reference external" href="https://www.dlsi.ua.es/~japerez/materials/transformers/attention">esta p√°gina</a> sobre el modelo transformer y el cap√≠tulo correspondiente del libro. En realidad, ya estudiaste para la sesi√≥n anterior todos estos conceptos, pero se te pidi√≥ que no te detuvieras en exceso en los detalles t√©cnicos del libro. Ahora, es el momento de que vuelvas a leerlo con m√°s calma y consultes tambi√©n las anotaciones del profesor que hay en la p√°gina web. En total, esta parte deber√≠a llevarte unas 3 horas üïíÔ∏è de trabajo.</p></li>
<li><p>Ampliar el estudio del transformer con la arquitectura codificador-descodificador completa, as√≠ como con la basada solo en codificador siguiendo para ello las secciones 6.1 a 6.5 de <a class="reference external" href="https://www.dlsi.ua.es/~japerez/materials/transformers/attention2/">esta p√°gina</a>. En total, esta parte deber√≠a llevarte unas 3 horas üïíÔ∏è de trabajo.</p></li>
<li><p>Realiza el <a class="reference external" href="https://forms.gle/qJMmKi6KGhtKDJtYA">test de evaluaci√≥n</a> de estos contenidos. Son pocas preguntas y te llevar√° unos minutos.</p></li>
</ul>
<p><strong><span style="font-size: 1.15em">Contenidos para la sesi√≥n del 10/05/2023</span></strong></p>
<p>En la clase presencial (3 horas üïíÔ∏è de duraci√≥n), repasaremos los contenidos de la semana anterior y veremos c√≥mo se <a class="reference external" href="https://www.dlsi.ua.es/~japerez/materials/transformers/implementacion/#codigo-para-el-transformer">implementa el modelo transformer en PyTorch</a>.</p>
<p><strong>Ejercicios</strong>: para este bloque, haz los siguientes ejercicios. Repasa las normas que se indican m√°s arriba sobre c√≥mo entregar los ejercicios.</p>
<ol class="simple">
<li><p><strong>Modelos de lengua basados en transformers</strong>: l√©ete este tutorial <a class="reference external" href="https://pub.towardsai.net/multi-query-attention-explained-844dfc4935bf">sencillo</a> y este otro m√°s <a class="reference external" href="https://blog.fireworks.ai/multi-query-attention-is-all-you-need-db072e758055">avanzado</a> sobre el concepto de multi-query attention y, a continuaci√≥n, modifica el c√≥digo del modelo de lengua basado en transformers para que use multi-query attention sin implementar todav√≠a una cach√© KV. Realiza un peque√±o estudio sobre c√≥mo afecta esto a la calidad del modelo; para esto puedes medir qu√© probabilidad da el modelo a algunas frases similares a las del conjunto de entrenamiento. Adicionalmente, estudia c√≥mo afecta a la calidad del modelo el uso de una cach√© KV que tendr√°s que implementar. Aunque ser√≠a deseable poder medir el impacto de ambas cosas en los tiempos de ejecuci√≥n, no es necesario que lo hagas, ya que probablemente no puedas medirlo con precisi√≥n suficiente salvo que incrementes el tama√±o de los datos de entrenamiento y el n√∫mero de par√°metros del modelo. Explica en tu respuesta las ideas b√°sicas tanto de multi-query attention como de la cach√© KV. <em>Nota:</em> este ejercicio es m√°s complejo que los anteriores por lo que requerir√° un mayor esfuerzo por tu parte; la nota final de tu informe se basar√° un 35% en los ejercicios de la sesi√≥n 1, un 35% en los de la sesi√≥n 2 y un 30% en los de esta sesi√≥n, por lo que se puede considerar que es un ejercicio <em>para nota</em>. Puedes usar, adem√°s, el doble de espacio para responder a este ejercicio.</p></li>
</ol>
<!--

**<span style="font-size: 1.15em">Contenidos pr√°cticos a trabajar tras la sesi√≥n</span>**

Tras la sesi√≥n, ya puedes ponerte a trabajar en la pr√°ctica de desarrollo a realizar para este bloque y de la cual saldr√° la mayor parte de la nota del bloque (un 90% aproximadamente). Se espera que dediques a ella unas 11 horas üïíÔ∏è de trabajo. La pr√°ctica se basa en modificar ligeramente el c√≥digo de [minGPT][guiamingpt] para poder realizar experimentos de interpretabilidad mecanicista. El enunciado completo est√° en el siguiente apartado.

## Pr√°ctica sobre interpretabilidad mecanicista de transformers

La *interpretabilidad mecanicista* en el contexto de la inteligencia artificial intenta dar una explicaci√≥n motivada del funcionamiento de los modelos de aprendizaje autom√°tico. Es una propuesta muy importante de cara a generar confianza en los sistemas e inducir ciertos comportamientos en ellos. Dentro del campo de la interpretabilidad mecanicista existen un buen n√∫mero de t√©cnicas que se pueden aplicar a los transformers. Aqu√≠ nos centraremos en el [parcheado de activaciones][patching].

[patching]: https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=qeWBvs-R-taFfcCq-S_hgMqx

El parcheado de activaciones *interviene* en una activaci√≥n espec√≠fica de un modelo mediante la sustituci√≥n de una activaci√≥n *corrompida* con una activaci√≥n *limpia*. Se mide entonces c√≥mo afecta el cambio a la salida del modelo. Esto nos permite identificar qu√© activaciones son importantes para el resultado del modelo y localizar posibles causas de errores en la predicci√≥n. 

En nuestro caso particular, vas a escribir c√≥digo que ejecute la versi√≥n m√°s peque√±a de GPT2 (usa la cadena `gpt2` en el c√≥digo) con dos entradas diferentes: dos textos que solo se diferencien en un √∫nico token. La idea es que al proporcionar al modelo la entrada corrompida, intervendremos en el embedding tras una cierta capa (uno solo cada vez) y lo parchearemos con el embedding correspondiente de la ejecuci√≥n limpia. Luego mediremos cu√°nto cambia la predicci√≥n del siguiente token respecto a la ejecuci√≥n limpia. Si el cambio es significativo, entonces podemos estar seguros de que la activaci√≥n que hemos alterado es importante para la predicci√≥n. Este proceso de parcheado lo realizaremos para cada capa del modelo y para cada token de la entrada. Con toda esta informaci√≥n, obtendremos una gr√°fica y sacaremos conclusiones. Por motivos que entender√°s en un momento, los dos textos han de tener el mismo n√∫mero de tokens.

**<span style="font-size: 1.15em">Ejemplo de an√°lisis</span>**

Daremos un ejemplo para que se entienda mejor. Considera el siguiente texto de entrada: "Michelle Jones was a top-notch student. Michelle". Si se lo damos a GPT2 y estudiamos la probabilidad emitida por el modelo para el token que sigue a la segunda aparici√≥n de Michelle, obtendremos lo siguiente (solo se muestran los 20 tokens m√°s probables): 

```{list-table}
:header-rows: 1

* - Position
  - Token index
  - Token
  - Probability
* - 1
  - 373
  - was
  - 0.1634
* - 2
  - 5437
  - Jones
  - 0.1396
* - 3
  - 338
  - 's
  - 0.0806
* - 4
  - 550
  - had
  - 0.0491
* - 5
  - 318
  - is
  - 0.0229
* - 6
  - 290
  - and
  - 0.0227
* - 7
  - 11
  - ,
  - 0.0222
* - 8
  - 531
  - said
  - 0.0134
* - 9
  - 468
  - has
  - 0.0120
* - 10
  - 635
  - also
  - 0.0117
* - 11
  - 1625
  - came
  - 0.0091
* - 12
  - 1297
  - told
  - 0.0084
* - 13
  - 1422
  - didn
  - 0.0070
* - 14
  - 2993
  - knew
  - 0.0067
* - 15
  - 1816
  - went
  - 0.0061
* - 16
  - 561
  - would
  - 0.0061
* - 17
  - 3111
  - worked
  - 0.0055
* - 18
  - 750
  - did
  - 0.0054
* - 19
  - 2486
  - Obama
  - 0.0053
* - 20
  - 2492
  - wasn
  - 0.0050

```

Como era de esperar, el token "Jones" tiene una probabilidad notablemente elevada. Ahora, considera la entrada corrompida "Michelle Smith was a top-notch student. Michelle". Si le damos esta entrada a GPT2, esperamos que la probabilidad de "Jones" a como continuaci√≥n del texto sea mucho menor que antes y que la de "Smith" sea mucho mayor, lo que (puedes comprobarlo) efectivamente ocurre. Pero queremos ir m√°s all√° y saber qu√© embeddings son los que m√°s influyen en esta diferencia. Dado que ambas entradas tienen 11 tokens (m√°s adelante explicaremos c√≥mo averiguarlo) y que el transformer del modelo GPT2 peque√±o tiene 12 capas, si nos centramos en los embeddings que se obtienen a la salida de cada capa, podemos parchear 11√ó12 = 132 embeddings diferentes. Calcularemos, por tanto, 132 veces la diferencia entre el logit de "Smith" y el logit de "Jones" en la salida del √∫ltimo token de la entrada ("Michelle") en el modelo corrompido. Observa que tambi√©n podr√≠amos calcular las diferencias tras aplicar la funci√≥n softmax, pero en este caso no lo haremos.

Una representaci√≥n en forma de mapa de calor del resultado es la siguiente:

```{figure} images/mechanistic-michelle.png
---
height: 540px
name: fig-mech
---
```

Recuerda que en un gr√°fico como el anterior, debido a la m√°scara de atenci√≥n y a la disposici√≥n de las capas, la informaci√≥n fluye de izquierda a derecha y de arriba a abajo. Puedes ver c√≥mo intervenir en la primera columna no tiene efectos en la predicci√≥n del siguiente token, lo que tiene todo el sentido, ya que los embeddings que se parchean tienen exactamente los mismos valores en el modelo limpio y en el corrompido, ya que el contexto anterior es el mismo. Tampoco parece haber cambios al parchear los embeddings de la tercera a la antepen√∫ltima columna. Sin embargo, observa c√≥mo al intervenir los embeddings de muchas capas del segundo token, la predicci√≥n se decanta hacia "Jones" (el color se hace oscuro cuando la diferencia entre el logit de "Smith" y el de "Jones" se va haciendo negativa porque "Jones" tiene un logit mayor). Modificar los embeddings de las √∫ltimas capas del segundo token tiene efectos mucho menores, ya que el embedding apenas puede influir en el futuro de la secuencia. En la √∫ltima posici√≥n ("Michelle") se observa que los embeddings de las capas finales van anticip√°ndose al token que tienen que predecir.

Algunos textos corrompidos adicionales que puede ser interesante explorar son, por ejemplo, "Jessica Jones was a top-notch student. Michelle" o "Michelle Smith was a top-notch student. Jessica".

En esta pr√°ctica se trata de que programes el c√≥digo que te permite obtener gr√°ficas y probabilidades como las anteriores, propongas tus propios textos limpios y corrompidos (intenta tirar de creatividad y no estudiar textos o fen√≥menos muy similares), realices un an√°lisis parecido al anterior y escribas un informe dentro de un cuaderno de Python de unas 1500-2000 palabras en el que presentes y comentes el c√≥digo que has implementado, adem√°s de presentar tu enfoque, los resultados y las conclusiones pertinentes. Ser√°n bienvenidas las ideas originales y los experimentos adicionales que se te ocurran.

**<span style="font-size: 1.15em">Tokenization</span>**

El modelo GPT2 usa un tokenizador basado en BPE que trocea el texto de entrada en palabras o en unidades inferiores dependiendo de su frecuencia. El c√≥digo de minGPT permite descargar dicho tokenizador y usarlo para segmentar los textos. El siguiente c√≥digo muestra c√≥mo tokenizar un texto para obtener sus √≠ndices y viceversa.

```python
from mingpt.bpe import BPETokenizer

input = "Michelle Jones was a top-notch student. Michelle"
print("Input:", input)
bpe = BPETokenizer()
# bpe() gets a string and returns a 2D batch tensor 
# of indices with shape (1, input_length)
tokens = bpe(input)[0]
print("Tokenized input:", tokens)
input_length = tokens.shape[-1]
print("Number of input tokens:", input_length)
# bpe.decode gets a 1D tensor (list of indices) and returns a string
print("Detokenized input from indices:", bpe.decode(tokens))  
tokens_str = [bpe.decode(torch.tensor([token])) for token in tokens]
print("Detokenized input as strings: " + '/'.join(tokens_str))
```

**<span style="font-size: 1.15em">Detalles de implementaci√≥n</span>**

Lo siguiente son algunos detalles de implementaci√≥n que te pueden ser √∫tiles, pero que no es necesario que sigas. 

Para conseguir un c√≥digo que te permita realizar el parcheado de activaciones te tendr√°s que centrar en los ficheros `mingpt/model.py` y `generate.ipynb`. Si trabajas en local sin usar un *notebook* (recomendado) copia el c√≥digo de `generate.ipynb` en un fichero `generate.py` que puedas ejecutar desde la l√≠nea de √≥rdenes.

Puedes trabajar directamente en una sesi√≥n de Google Colab. Aqu√≠ tienes un [proyecto][proyectocolab] (accede con tu cuenta de `gcloud.ua.es`) con instrucciones sobre c√≥mo usarlo para desarrollar. Sin embargo, es mucho m√°s c√≥modo desarrollar en local (entre otras cosas, puedes trabajar con un mejor editor de texto que el de Colab y tambi√©n depurar). Incluso si no tienes una GPU, el c√≥digo funciona sin problemas sobre CPU y solo tarda unos segundos m√°s que sobre GPU al solo trabajar con un texto y con un modelo no excesivamente grande. Cuando tengas el c√≥digo final, puedes subirlo a un notebook para su entrega.

A√±ade a la funci√≥n `forward` del transformer, c√≥digo que permita salvar (seg√∫n el valor de cierto *flag* booleano recibido como par√°metro) en una variable de instancia las activaciones de cada capa y cada posici√≥n. Recuerda hacer una copia profunda de los embeddings y no guardar √∫nicamente una referencia que puede ser sobreescrita posteriormente; para ello, consulta la secuencia de llamadas `.detach().clone()` de PyTorch. A√±ade tambi√©n c√≥digo que permita (de nuevo en base a un par√°metro booleano) parchear el embedding de una capa y posici√≥n concretas. 

A√±ade tambi√©n a la funci√≥n `forward` c√≥digo que guarde los logits del √∫ltimo token, que contienen la informaci√≥n que nos interesa sobre la predicci√≥n del siguiente token. Puedes guardar esta informaci√≥n en un atributo que luego puedes acceder desde el exterior de la clase. Observa que solo te interesa el vector correspondiente al √∫ltimo token.

A√±ade c√≥digo al fichero `generate.py` que divida el texto limpio en tokens, lo pase por el modelo a trav√©s de la funci√≥n `generate` (pidi√©ndole al modelo que guarde los embeddings intermedios) y muestre las continuaciones m√°s probables a partir de los logits del √∫ltimo token. Ten en cuenta que si quieres saber la probabilidad de una continuaci√≥n como el token "Jones", por ejemplo, has de buscar el √≠ndice de dicho token en el vocabulario anteponi√©ndole un espacio en blanco (`index = bpe(' Jones')`). Esto es as√≠ porque el segmentador de BPE trata de forma diferente los tokens que aparecen al principio de la secuencia y los que aparecen en medio. Una vez tengas el √≠ndice del token, puedes acceder a la posici√≥n correspondiente del vector de logits y obtener la probabilidad no normalizada de que sea la continuaci√≥n.

Despu√©s, puedes trabajar con el texto corrupto. Incluye un doble bucle que itere sobre todas las capas y todas las posiciones y llame cada vez a `generate` pas√°ndole la capa y la posici√≥n en la que realizar la intervenci√≥n. En cada paso, eval√∫a la diferencia de logits oportuna y gu√°rdala en una matriz de diferencias.

Usa finalmente la funci√≥n `matshow` de `matplotlib` para visualizar la matriz de diferencias.

[proyectocolab]: https://colab.research.google.com/drive/1dq2EClvIbEtoEnHWoAXZQTArJDHivQly?usp=sharing

**<span style="font-size: 1.15em">Una explicaci√≥n m√°s informal</span>**

La siguiente explicaci√≥n informal puede que te ayude a entender mejor el objetivo de la pr√°ctica.

Considera para simplificar la frase "a b c" y la versi√≥n corrompida "d e f". En general, habr√° muchos m√°s tokens en com√∫n, pero as√≠ queda todo m√°s claro en la siguiente discusi√≥n. Considera que el modelo neuronal basado en el transformer tiene 5 capas de atenci√≥n. Considera que vamos a estudiar qu√© embeddings son importantes para la predicci√≥n de que tras estas frases vaya el token "X".

Se trata primero de que permitas que en la funci√≥n forward del transformer (clase `GPT`) se puedan guardar (por ejemplo en una lista de listas de tensores) los 3x5=15 embeddings que se generan a la salida de cada una de las capas cuando se procesa la frase "a b c". En el enunciado se dan algunos detalles porque no puedes guardar simplemente la referencia a los tensores, ya que se modificar√°n la pr√≥xima vez que llames a forward, sino que has de clonar los tensores (lo que se llama "copia defensiva"). Con esto tendr√°s almacenados los 15 tensores (embeddings) de la frase limpia.

Gu√°rdate tambi√©n los logits tras la √∫ltima capa. En particular, solo necesitar√°s los de la √∫ltima posici√≥n (es decir, los logits correspondientes al token "c"), que te dan una medida de la probabilidad del siguiente token, es decir, del token que ir√° tras "c". Recuerda que estos logits no son realmente probabilidades (son valores como -11.1, -0.5, 0.78, o 2.32323) porque no se les ha aplicado la funci√≥n softmax, pero trabajar con ellos es m√°s c√≥modo que trabajar con las probabilidades porque tenemos valores con un rango m√°s amplio. No obstante, el estudio podr√≠a hacerse igualmente con probabilidades estrictas. En realidad, ni siquiera necesitas guardarte todos los logits, sino solo el escalar que corresponde al token "X" porque es lo √∫nico que usar√°s despu√©s.

Ahora le das al modelo la versi√≥n corrompida "d e f", indic√°ndole que no sobreescriba la copia de los embeddings que obtuvimos con la frase limpia. La frase corrompida ha de tener el mismo n√∫mero de tokens que la limpia para que la siguiente discusi√≥n tenga sentido. La idea es modificar uno solo de los 15 embeddings que se producen mientras se procesa la frase sucia. Si, por ejemplo, nos centramos en el embedding del primer token ("d") tras la primera capa, se tratar√≠a de que el c√≥digo de la funci√≥n forward opere "casi" de la forma normal, pero cuando se obtenga la salida de la primera capa y antes de pasarla como entrada a la segunda capa, se ha de modificar el embedding correspondiente a la primera palabra (solo ese) y sustituirlo por el embedding correspondiente (de la misma capa y posici√≥n) que te guardaste para la frase limpia (es decir, en este caso, ser√≠a el embedding que te guardaste tras la primera capa para el token "a"). Con esto, la segunda capa recibir√° como entrada el embedding que se gener√≥ para "a" en lugar del de "d".

Tras intervenir en el embedding de la posici√≥n 1 tras la capa 1, el resto del modelo trabaja sin ning√∫n "contratiempo". De la misma manera que antes, ahora miramos los logits de la predicci√≥n del token que va tras el √∫ltimo token de la frase corrompida (es decir, "f"). Y nos centramos en el valor del logit de la predicci√≥n del token "X". La diferencia entre este valor y el que nos guardamos para la frase limpia nos da una idea de c√≥mo de relevante es el embedding de la capa 1 y posici√≥n 1 en la predicci√≥n del token "X". En el enunciado se muestra c√≥mo algunos embeddings son mucho m√°s relevantes que otros. Y t√∫ tienes que hacer un estudio similar con diferentes frases.

Si repites la operaci√≥n anterior con los otros 14 embeddings (llamando 14 veces m√°s a la funci√≥n forward), terminar√°s teniendo 15 diferencias de logits (15 valores escalares) que puedes representar en un mapa de calor de 3x5 como se ve m√°s arriba.

Finalmente, ten en cuenta que la discusi√≥n de este apartado tiene una peque√±a simplificaci√≥n respecto a lo que se pide en el enunciado de m√°s arriba. All√≠ se propon√≠a calcular la diferencia entre el logit de "Smith" y el logit de "Jones" en la salida del √∫ltimo token en el modelo corrompido, lo que da un poco m√°s de informaci√≥n que la diferencia que hemos explicado en este apartado, es decir, la diferencia entre la predicci√≥n de un solo token ("Jones") en la frase limpia y la corrompida, en lugar de dos tokens en la frase corrompida. En realidad, cualquiera de las dos opciones es v√°lida para llegar a las conclusiones que nos interesan: que en la frase corrompida, el logit de "Jones" se hace mucho menor excepto para ciertas intervenciones. Si quieres que tu mapa de calor coincida con el de este enunciado, sigue el enfoque basado en los dos tokens "Jones" y "Smith".

**<span style="font-size: 1.15em">Ampliar conocimientos</span>**

Lo anterior es solo uno de los m√∫ltiples an√°lisis que se han propuesto dentro de la interpretabilidad mecanicista. Para esta pr√°ctica no se espera que vayas m√°s all√° de esto, pero si te interesa conocer un par de an√°lisis m√°s puedes consultar [este tutorial][lines50]. Observa que aunque el tutorial usa una librer√≠a para parchear las activaciones, en esta pr√°ctica no puedes usar ninguna librer√≠a para ello y lo has de hacer directamente sobre el c√≥digo de minGPT. Una revisi√≥n mucho m√°s detallada sobre la interpretabilidad mecanicista se puede encontrar en [este trabajo][nanda] de Neel Nanda.

[lines50]: https://www.lesswrong.com/posts/hnzHrdqn3nrjveayv/how-to-transformer-mechanistic-interpretability-in-50-lines
[nanda]: https://www.neelnanda.io/mechanistic-interpretability/glossary

-->
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="bloque3_ev.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title"><span class="section-number">17. </span>Ev. Evaluaci√≥n de pr√°cticas del Bloque 2</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="doc_utils/creditosColab/web.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">19. </span>Uso de Google Colab con una VM personalizada</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Universitat d'Alacant<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>